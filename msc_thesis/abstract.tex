Deep Reinforcement Learning methods on mechanical control are successful on many environments and used instead of traditional optimal and adaptive control methods on some complex cases. However, Deep Reinforcement Learning algorithms do still have challenges. One is control on partially observable environments. When an agent is not informed well about the environment, it must recover information from past observations. In this thesis, walking of Bipedal Walker (OpenAI GYM) environment is studied by continious actor-critic reinforcement learning algorithm Twin Delayed Deep Determinstic Policy Gradient. Environment is partially observable because walker is not able to see behind. Several neural architectures are implemented. First one is Residual Feed Forward Neural Network under the observable environment assumption, while second and third ones are Long Short Term Memory and Transformer using observation history as input to recover hidden state since environment is assumed to be partially observable.



