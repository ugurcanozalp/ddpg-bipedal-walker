\section{Proposed Methods and Contribution}
\label{sec:proposed_methods}

Partially observable environments are always a hard work for reinforcement learning algorithms. 
In this work, the walker environment is assumed to be fully observable environment at first. 
A Residual Feed-Forward Neural Network architecture is proposed to control 
the robot under fully observability assumption due to the fact that no memory is used during decision making. 
Then, the environment is assumed to be partially observable. 
In order to recover belief states, Long Short Term Memory (LSTM) and 
Transformer neural networks are proposed using fixed number of 
past observations (4 in our case) during decision making. 

LSTM is used in many deep learning applications including sequential data. 
It is a variant of Recurrent Neural Networks (RNN) and a good candidate for RL algorithms to be applied in partially observable environments. 

Transformer is developed to handle sequential data as RNN models do. 
However, it processes the whole sequence at the same time, while RNN processes the sequence in order. 
Transformers are commonly used in Natural Language Processing (NLP) thanks to major performance improvements, however, this is not the case for Reinforcement Learning, yet.

In order to handle the reward sparsity problem, reward function is reshaped in this thesis. Also, a exploration strategy is formed so that the agent both explores and learns sufficiently well. 

In this thesis, as a RL algorithm, Twin Delayed Deep Deterministic Policy Gradient (TD3) and Soft Actor Critic (SAC) are chosen. 
TD3 is an improved version of Deep Deterministic Policy Gradient (DDPG) algorithm while SAC is a stochastic type of RL methods and lets the agent to learn how much to explore depending on observations.
