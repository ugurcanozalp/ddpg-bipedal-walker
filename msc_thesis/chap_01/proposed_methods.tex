\section{Proposed Methods and Contribution}
\label{sec:proposed_methods}

Partially observable environments are always a hard work for reinforcement learning algorithms. 
In this work, the walker environment is assumed to be fully observable environment at first. 
A Residual Feed-Forward Neural Network architecture is proposed to control 
the robot under fully observability assumption due to the fact that no memory is used during decision making. 
Then, the environment is assumed to be partially observable. 
In order to recover belief states, Long Short Term Memory (LSTM) and 
Transformer neural networks are proposed using fixed number of 
past observations (6 and 12 in our case) during decision making. 

LSTM is used in many deep learning applications including sequential data. 
It is a variant of Recurrent Neural Networks (RNN) and a good candidate for RL algorithms to be applied in partially observable environments. 

Transformer is developed to handle sequential data as RNN models do. 
However, it processes the whole sequence at the same time, while RNN processes the sequence in order. 
Transformers are commonly used in Natural Language Processing (NLP) thanks to major performance improvements over RNN variants, but this is not the case for Reinforcement Learning, yet.

In order to handle the reward sparsity problem, reward function is redesigned in this thesis. Also, an exploration strategy is formed so that the agent both explores and learns sufficiently well. 

In this thesis, Twin Delayed Deep Deterministic Policy Gradient (TD3) and Soft Actor Critic (SAC) are used to solve our environment as RL algorithm. 
TD3 is a deterministic RL method with additive exploration noise, and it is improved version of Deep Deterministic Policy Gradient (DDPG).
SAC is a stochastic type of RL method with adaptive exploration. 
It adjusts how much to explore depending on observations and rewards.
