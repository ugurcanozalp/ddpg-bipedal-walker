\section{Related Work in Literature}
\label{sec:related_work}

Reinforcement Learning methods are used in many mechanical control tasks 
such as autonomus driving \cite{pan_virtual_2017, shalev-shwartz_safe_2016, sallab_deep_2017, wang_deep_2019} 
and autonomus flight \cite{kopsa_reinforcement_2018, abbeel_application_2006, santos_experimental_2012}, where conventional control methods are difficult to implement.

Rastogi \cite{rastogi_deep_2017} used Deep Deterministic Policy Gradient (DDPG) algorithm to walk 
their physical bipedal walker robot along with simulation environment. 
They concluded that DDPG is infeasible to control the walker robot 
since it requires long time for convergence. 
Kumar et al. \cite{kumar_bipedal_2018} also used DDPG to perform 
robot walking in 2D simulation environment. 
Their agent achieved desired score in approximately 25,000 episodes. 
Song et al. \cite{song_recurrent_2018} pointed out the partial observability problem of bipedal walker, 
using Recurrent Deep Deterministic Policy Gradient (RDDPG)~\cite{heess_memory-based_2015} algorithm 
and acquired better results than the original DDPG algorithm. 
Haarnoja et al. \cite{haarnoja_learning_2019} used maximum entropy learning for gaiting of real robot and achieved stable gait in 2 hours. 

Fris \cite{fris_landing_2020} used Twin Delayed Deep Deterministic Policy Gradient (TD3) using LSTM for their quadrocopter landing task in sparse reward setting.
They stated that LSTM is used to infer possible high order dynamics from observations.  
Fu et al. \cite{fu_when_2020} used vanilla RNN with attention mechanism 
using TD3 for car driving task, but not explicit Transformer. 
They reported that their method outperformed seven baselines. 
Upadhyay et al. \cite{upadhyay_transformer_2019} used Feed Forward Neural Network, 
LSTM and vanilla Transformer architectures for balancing pole 
on a cart from Cartpole environment of Gym, and Transformer yield worst results among three architectures. 
Parisotto et al. \cite{parisotto_stabilizing_2019} pointed out order of layer normalization with respect to attention and feed-forward layers dramatically changes performance on RL tasks.
 