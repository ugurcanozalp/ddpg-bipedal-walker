\section{Proposed Methods and Contribution}
\label{sec:proposedmethods}
Partially observable environments are always a hardwork for reinforcement learning algorithms. In this work, the walker environment assumed as fully observable environment at first. A feed-forward neural network architecture is proposed to control the robot. Then, the environment is assumed to be partially observable. In order to recover latent states, Long Short Term Memory (LSTM) and Transformer neural networks are proposed. \\
LSTMs are used in many deep learning applications which includes sequential data. It is a variant of Recurrent Neural Networks (RNN). It is a good candidate for RL algorithms which solves partially observable environments. \\
Transformers are developed to handle sequential data as RNN models does. However, it processes the whole sequence at same time, while RNNs process the sequence in order. It replaced RNNs in Natural Language Processing (NLP) tasks over RNNs due to its major improvements. However, this is not the case for Reinforcement Learning, yet.
As RL algorithm, Twin Delayed Deep Deterministic Policy Gradient (TD3) is used. It is improved version of Deep Deterministic Policy Gradient (DDPG) Algorithm.
