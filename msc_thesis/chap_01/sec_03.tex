\section{Related Work}
\label{sec:relatedwork}
Reinforcement Learning methods are used in many mechanical control tasks such as autonomus driving  \cite{pan_virtual_2017} \cite{shalev-shwartz_safe_2016} \cite{sallab_deep_2017} \cite{wang_deep_2019} and autonomus flight   \cite{kopsa_reinforcement_2018} \cite{abbeel_application_2006} \cite{santos_experimental_2012}. \\
Rastogi \cite{rastogi_deep_2017} used Deep Deterministic Policy Gradient (DDPG) algorithm to walk their physical bipedal walker robot along with simulation environment. They concluded that DDPG is infeasible to control walker robot since it requires long time for convergence. Kumar et al. \cite{kumar_bipedal_2018} also used DDPG to perform robot walking in 2D simulation environment. Their agent converged in approximately 25k episodes. Song et al. \cite{song_recurrent_2018} pointed out the partial observability problem of bipedal walker, using Recurrent Deep Deterministic Policy Gradient (RDPG) \cite{heess_memory-based_2015} algorithm and acquired better results than original Deep Deterministic Policy Gradient (DDPG) algorithm. \\
Fris \cite{fris_landing_2020} used Twin Delayed DDPG (TD3) using Long Short Term Memory (LSTM) for their quadrocopter landing task. Fu et al. \cite{fu_when_2020} used vanilla RNN with attention mechanism using TD3 for car driving task, but not explicit Transformer. They reported that their method outperformed 7 baselines. Upadhyay et al. \cite{upadhyay_transformer_2019} used all of feed forward network, LSTM an original Transformer architectures for balancing pole on a cart from Cartpole environment of Gym, and Transformer yield worst results among three architectures.