\chapter{REINFORCEMENT LEARNING}
\label{chap:rl_chap}

Machine Learning is ability of computer program which allows 
adaptation to new situations through experience, 
without explicitly programmed~\cite{mitchell_machine_1997}. 
There exist three main paradigm. 

\textbf{Supervised Learning} is task of learning a function $f \colon X \rightarrow Y$ 
that maps an input to an output based on $N$ example input-output pairs $(x_i,y_i)$ 
such that $ f(x_i) \approx y_i$, for all $i \in {1,2,...,N}$ 
by minimizing error between predicted and target output. 
Input $x$ can be thought as state of an agent. 
That makes $y$ correct action at state $x$. 
For supervised learning, both $x$ and $y$ should be available, 
where the correct action are provided by a friendly supervisor~\cite{russell_artificial_nodate}. 

\textbf{Unsupervised Learning} is discovering structure on input examples without any label on it. 
Based on $N$ example input pairs $(x_i)$, 
it is discovering function $f \colon X \rightarrow Y$, $ f(x_i) = y_i$, for all $i \in {1,2,...,N} $, 
where $y_i$ is discovered output. 
This discovery is motivated by predefined objective but this is not target error as in Supervised Learning. 
This objective is maximization of a value which represents compactness of output representations. 
Again, input $x$ can be thought as state of an agent. 
However, correct action is not available and there is no given hint in this case. 
It can learn relations among states but it does not know what to do 
since there is no target or utility~\cite{russell_artificial_nodate}.
 
\textbf{Reinforcement Learning} is one of three main machine learning paradigm along with Supervised and Unsupervised Learning. 
It is closest kind of learning demonstrated by humans and animals 
since it is grounded by biological learning systems. 
It is based on maximizing cumulative reward over time to make agent 
learn how to act in an environment~\cite{sutton_reinforcement_1998}. 
Each action of agent is either rewarded or punished according to a reward criteria. 
Therefore, reward function is mathematical representation of what to teach agent. 
The agent explores environment by taking various actions in different states to get experience, based on trial-and-error. 
Then it exploits experiences to get highest reward from environment considering instant and future rewards over time. 

Formally, Reinforcement Learning is learning a policy function 
$\pi \colon \mathcal{S} \rightarrow \mathcal{A}$ which maps 
inputs (observations) $s \in \mathcal{S}$ to outputs (actions) $a \in \mathcal{A}$. 
Learning is maximization of value function $V^{\pi}(s)$ (cumulative reward) for all possible states, which depends on policy $\pi$. 
Being so, this is similar to unsupervised learning. 
However, the difference is that value function $V^{\pi}(s)$ is not defined exactly unlike unsupervised learning. 
It is also learned by interacting with environment by taking all possible actions in all possible states. 

Reinforcement Learning is different than Supervised Learning because correct actions are not provided. 
Meanwhile, it is also different than Unsupervised Learning because the agent is forced to learn a behaviour. 
The agent is evaluated at each time step without supervision.