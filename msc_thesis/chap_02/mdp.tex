\section{Markov Decision Process}
\label{sec:mdp}

Markov Decision Process (MDP) is a sequential decision making process with Markov property. 
It is represented as a tuple $(\mathcal{S},\mathcal{A},T,R,\gamma)$. 
Markov property means that the conditional probability distribution of the future state depends only on the instant state and action instead of the entire past, so it is memoryless. 
In MDP setting, the system is fully observable which means states can be derived from instant observations; i.e., $s_t=f(o_t)$. 
Therefore, agent can decide action based on only instant observation $o_t$ instead of what happened on previous time \cite{francois-lavet_introduction_2018}. 

\begin{description}
	\item[State Space $\mathcal{S}$] A set of all possible configurations of system. 
	
	\item[Action Space $\mathcal{A}$]  A set of all possible actions of agent. 
	
	\item[Model $T \colon \mathcal{S} \times \mathcal{S} \times \mathcal{A} \mapsto \lbrack 0,1 \rbrack$] Function of how environment evolves through time, representing transition probabilities as $T(s'|s,a) = p(s'|s,a)$ 
	where $s' \in \mathcal{S}$ is next state, $s \in \mathcal{S}$ is instant state and $a \in \mathcal{A}$ is taken action.
	
	\item[Reward Function $R \colon \mathcal{S} \times \mathcal{A} \mapsto \mathbb{R}$] Function of rewards coming from environment. 
	At each state transition $s_t \rightarrow s_{t+1}$, a reward $r_t$ is given to agent. 
	Rewards can be either deterministic or stochastic. 
	Reward function is the expected value of reward at given state $s$ and taken action $a$, defined by,
	\begin{equation}
	R(s,a) = \mathbb{E}[r_t|s_t=s, a_t=a]. %\: \forall t = 0,1, ...
	\end{equation}
	
	\item[Discount Factor $\gamma \in \lbrack 0,1 \rbrack$] Measure of the importance of rewards in the future for value function.
\end{description}
