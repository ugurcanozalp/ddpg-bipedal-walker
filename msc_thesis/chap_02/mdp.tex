\section{Markov Decision Process}
\label{sec:mdp}

Markov Decision Process (MDP) is a control process with reward function and discount factor. It is represented as a tuple $(S,A,P,R,\gamma)$. Markov property means that the conditional probability distribution of the future state depends only on the instant state and action instead of the entire past. 

\textbf{State}: State $s \in S$

\textbf{Acgtion}: Action $s \in S$

\textbf{Model}: Model is mathematical representation of how environment evolves through time, including transition probabilities $p(s'|s,a)$ where $s' \in S$ is next state, $s \in S$ is instant state and $a \in A$ is taken action.

\textbf{Reward}: Reward $R \colon S \times A \mapsto \mathbb{R}$ is a function of state and action. It is ultimate utility function of Reinforcement Learning.

\textbf{Discount Factor and Return}: xxx Horizon?

\textbf{Policy}: It is a mapping $\pi \colon S \mapsto A$ which maps states to actions. 

\subsection{Value Functions}

\subsubsection{State Value Function}

\subsubsection{State-Action Value Function}
