\section{Model Free Reinforcement Learning}
\label{sec:mf_rl}

Model based methods are based on solving bellman equation \ref{eqn:bellman_v}\ref{eqn:bellman_q} with given model $T$. On the other hand, Model Free Reinforcement Learning is suitable if environment model is not available but agent can experience environment by consequences of its actions. There are 3 main types.

\textbf{Value Based Learning}: Value functions are learned, then policy arises naturally from value function as shown in \ref{eqn:policy_stochastic_q} \ref{eqn:policy_deterministic_q}. Since argmax operation is used, this type of learning is suitable for problems where action space is discrete.

\textbf{Policy Based Learning}: Policy is learned directly, return values are used instead of learning a value function. Unlike value based methods, it is suitable for continious action spaces.

\textbf{Actor Critic Learning}: Both policy (actor) and value (critic) functions are learned simulatenously. It is also suitable for continious action spaces. 

\subsection{Q Learning}
Q Learning is a value based type of learning. It is based on optimizing $Q$ function using bellman equation \ref{eqn:bellman_q} \cite{watkins_technical_1992}. 

Assume that $Q$ function is parametrized by $\theta$. Target $Q$ value is estimated by bootstrapping estimate itself, as shown in  \eqref{eqn:q_target}. 
%
\begin{equation}
\label{eqn:q_target}
Y_t^Q = r_t + \gamma \max_{a'} Q(s_{t+1},a';\theta)
\end{equation}

At time $t$,  with state, action, reward, next state tuples ($s_t,a_t,r_t,s_{t+1}$), $Q$ values are updated by minimizing difference between target value and estimated value with respect to $\theta$ using numerical optimization methods.

\begin{equation}
\label{eqn:q_loss}
\mathcal{L}_t(\theta) = \big( Y_t - Q(s_t,a_t;\theta) \big) ^ 2
\end{equation}

\subsubsection{Deep Q Learning}

Deep Q Learning overcomes instability of Q Learning. When a nonlinear approximator is used, learning is unstable. Deep Q Learning introduces Target Network and Experience Replay \cite{mnih_human-level_2015, mnih_playing_2013}. 

\textbf{Target Network}: Target network is parametrized $\theta^-$. It is used to evaluate target value and not updated by loss minimization. It is updated at each fixed number of update step $C$ by Q network parameter $\theta$. Target value is obtained by using $\theta^-$.

\begin{equation}
\label{eqn:dqn_ntarget}
Y_t^{DQN} = r_t + \gamma \max_{a'} Q(s_{t+1},a';\theta^-)
\end{equation}

\textbf{Experience Replay}: Experience tuples are stored in dataset $\mathcal{D}$ as queue with fixed buffer size $N_{replay}$. At each iteration $i$, $\theta$ is updated by experiences uniformly subsampled from experience replay. It allows agent to learn from experiences multiple times. More importantly, sampled experiences are close to be independent and identically distributed if buffer size is high enough. This makes learning process more stable.  

\begin{equation}
\label{eqn:dqn_loss}
\mathcal{L}_i(\theta_i) = \mathbb{E}_{s,a,r,s'\sim U(\mathcal{D})}\Big[\big( Y^{DQN} - Q(s,a;\theta_i) \big) ^ 2 \Big]
\end{equation}

\textbf{Epsilon Greedy Exploration}: As stated in \chapref{sec:chal}, exploration is an important step for reinforcement learning algorithms. In discrete action space (finite action space $\mathcal{A}$), simplest exploration strategy is epsilon-greedy approach. During learning, a random action is selected by probability $\epsilon$ or greedy action (maximizing Q value) by $1-\epsilon$ as shown in \eqref{eqn:egreedy_policy}. 
%
\begin{equation}
\label{eqn:egreedy_policy}
\pi(a|s) = 
\begin{cases}
1-\epsilon,   & \text{if } a = \argmax_{a} Q(s, a)\\
\frac{\epsilon}{|\mathcal{A}|-1},     & \text{otherwise}
\end{cases} 
\end{equation}

Algorithm is summarized in Algorithm \ref{alg:dqn}. 

\begin{algorithm}[H]
	\SetAlgoLined
	\DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead
	Initialize: Replay memory $\mathcal{D}$ with capacity $N_{replay}$ \\
	Action value function parameters $\theta$ \\
	Target action value function parameters $\theta^- \leftarrow \theta$ \\
	Epsilon parameter for exploration $\epsilon$, Update delay parameter $d$ \\
	\For{$\text{episode} = 1, M $}{
		Recieve initial state $s_1$; \\
		\For{$t = 1, T$}{
			Select random action $a_t$ with probability $\epsilon$, otherwise greedy action $a_t = \argmax_{a} Q(s_t, a; \theta)$; \\
			Execute action $a_t$ and recieve reward $r_t$ and next state $s_{t+1}$; \\
			Store experience tuple $e_t = (s_t, a_t, r_t, s_{t+1})$ to $\mathcal{D}$ ; \\
			Sample random batch with $N$ transitions from $\mathcal{D}$ as $\mathcal{D}_{r}$; \\
			Set $Y_j^{DQN} = \begin{cases}
			r_j + \gamma \max_{a'} Q(s_{j+1},a';\theta^-) & \text{if } s_{j+1} \text{ not terminal } \\
			r_j & \text{if } s_{j+1} \text{ terminal }
			\end{cases}
			\forall e_j \in \mathcal{D}_{r}$;
			Update $\theta$ by minimizing $ \frac{1}{N}\sum_{e_j \in \mathcal{D}_{r}} \big( Y_j^{DQN} - Q(s_j,a_j;\theta) \big) ^ 2$ with a single optimization step; \\
			\lIf{$t \mod d$}{
				Update target network: $\theta^- \leftarrow \theta$;
			}
		}
	}
	\caption{Deep Q Learning with Experience Replay}
	\label{alg:dqn}
\end{algorithm}

\subsubsection{Double Deep Q Learning}

In DQN, max operator is used to select and evaluate action on the same network \ref{eqn:dqn_ntarget}. This yields overestimated value estimations in noisy environments. Therefore, action selection and value estimation is decoupled in target evaluation to overcome $Q$ function overestimation \cite{van_hasselt_deep_2015}.

\begin{equation}
\label{eqn:ddqn_ntarget}
Y_t^{DDQN} = r_t + \gamma Q(s_{t+1}, \argmax_{a'} Q(s_{t+1}, a'; \theta_i );\theta^-)
\end{equation}

Learning process is same with DQN except target value.

\subsection{Deterministic Actor Critic Learning}
In value based methods are not suitable for continious action spaces. Therefore, policy is explicitly defined instead of maximizing $Q$ function. Policy function can be either stochastic or deterministic.  Deterministic actor-critic is kind of learning which uses deterministic policy \cite{silver_deterministic_2014}. It can be thought as continious version of Q Learning. Value function is called critic while policy function is called actor.

In discrete action space, policy is naturally obtained by argmax operation on $Q$ function as in \ref{eqn:policy_deterministic_q}. In DPG, policy $\mu$ is parametrized by another set of parameters $\theta^\mu$ while value function $Q$ is parametrized by $\theta^Q$. 

The ultimate goal is to maximize value function $V$. It is done by selecting action which maximizes $Q$ value as policy \eqref{eqn:policy_deterministic_q}. Therefore, value function is the criteria to be maximized by solving parameters $\theta^\mu$ given $\theta^Q$. 
%
\begin{equation}
\label{eqn:dpg_value_maximization}
\theta^\mu = \argmax Q(s_t, \mu(s_t;\theta^\mu);\theta^Q)
\end{equation}

In order to learn policy, $Q$ function should also be learned simulatenously. For $Q$ function approximation, target value is parametrized by $\theta^Q$ and $\theta^\mu$ \eqref{eqn:dpg_target}. And this target is used to learn $Q$ function by minimizing least squares loss \eqref{eqn:dpq_loss}.
%
\begin{equation}
\label{eqn:dpg_target}
Y_t^{DPG} = r_t + \gamma Q(s_{t+1}, \mu(s_{t+1};\theta^\mu);\theta^Q)
\end{equation}
%
\begin{equation}
\label{eqn:dpq_loss}
\mathcal{L}_t(\theta^Q) = \big( Y_t^{DPG} - Q(s_t,a_t;\theta^Q) \big) ^ 2
\end{equation}

Note that both $\theta^Q$ and $\theta^\mu$ should be learned at the same time. Therefore, parameters are updated simulatenously during learning iterations.

\subsubsection{Deep Deterministic Policy Gradient}
Deep Deterministic Policy Gradient is continuous compliment of Deep Q learning using deterministic policy \cite{lillicrap_continuous_2019}. It uses experience replay and target networks.  

Similar to Deep Q learning, there are target networks parametrized by $\theta^{\mu^-}$ and $\theta^{Q^-}$ along with main networks parametrized by $\theta^{\mu}$ and $\theta^{Q}$. While target networks are updated in fixed number of steps in DQN, DDPG updates target network parameters at each step with polyak averaging as follows:
%
\begin{equation}
\label{eqn:target_update}
\theta^- \leftarrow \tau \theta + (1-\tau) \theta^-
\end{equation}

The $\tau$ is an hyperparameter indicating how slow the target network is updated and usually close to zero.

Policy network parameters are learned by maximizing resulting expected value \ref{eqn:ddpg_value_maximization}. Note that value network parameters are assumed to be learned.

\begin{equation}
\label{eqn:ddpg_value_maximization}
\theta^\mu = \argmax \mathbb{E}_{s \sim U(\mathcal{D})} \Big[ Q(s, \mu(s_t;\theta^\mu);\theta^Q) \Big]
\end{equation}
 
Target networks are used to predict targe value \ref{eqn:ddpg_target}. This target is used to learn $Q$ function by minimizing least squares loss \ref{eqn:ddpq_loss} in each iteration.

\begin{equation}
\label{eqn:ddpg_target}
Y_t^{DDPG} = r_t + \gamma Q(s_{t+1}, \mu(s_{t+1};\theta^{\mu^-});\theta^{Q^-})
\end{equation}

\begin{equation}
\label{eqn:ddpg_loss}
\mathcal{L}_i(\theta_i) = \mathbb{E}_{s,a,r,s'\sim U(\mathcal{D})}\Big[\big( Y^{DDPG} - Q(s,a;\theta^Q_i) \big) ^ 2 \Big]
\end{equation}

In DDPG, value and policy network parameters are learned simultaneously. During learning, exploration noise is added to each selection. In original paper \cite{lillicrap_continuous_2019}, authors proposed to use Ornstein Uhlenbeck Noise \cite{uhlenbeck_theory_1930} which has temporal correlation for efficiency. However, simple gaussian noise or another noise is also possible.

Algorithm is summarized in Algorithm \ref{alg:ddpg}. 

\begin{algorithm}[H]
	\SetAlgoLined
	\DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead
	Initialize: Replay memory $\mathcal{D}$ with capacity $N_{replay}$ \\
	Policy and action value function parameters $\theta^{\mu}$, $\theta^Q$ \\
	Target policy and action value function parameters $\theta^{\mu^-} \leftarrow \theta^{\mu}$, $\theta^{Q^-} \leftarrow \theta^{Q}$ \\
	Random process $\mathcal{N}$ as exploration noise\\
	\For{$\text{episode} = 1, M $}{
		Recieve initial state $s_1$; \\
		\For{$t = 1, T$}{
			Select action $a_t = \mu(s_t; \theta^{\mu}) + \epsilon$ where $\epsilon \sim \mathcal{N}$; \\
			Execute action $a_t$ and recieve reward $r_t$ and next state $s_{t+1}$; \\
			Store experience tuple $e_t = (s_t, a_t, r_t, s_{t+1})$ to $\mathcal{D}$ ; \\
			Sample random batch with $N$ transitions from $\mathcal{D}$ as $\mathcal{D}_{r}$; \\
			Set $Y_j^{DDPG} = \begin{cases}
			r_j + \gamma Q(s_{j+1},\mu(s_{j+1}; \theta^{\mu^-}); \theta^{Q^-}) & \text{if } s_{j+1} \text{ not terminal } \\
			r_j & \text{if } s_{j+1} \text{ terminal }
			\end{cases}
			\forall e_j \in \mathcal{D}_{r}$; \\
			Update $\theta^Q$ by minimizing $ \frac{1}{N}\sum_{e_j \in \mathcal{D}_{r}} \big( Y_j^{DDPG} - Q(s_j,a_j;\theta^Q) \big) ^ 2$ with a single optimization step; \\
			Update $\theta^{\mu}$ by maximizing $ \frac{1}{N}\sum_{e_j \in \mathcal{D}_{r}} Q(s_j,a_j;\theta^Q)$ with a single optimization step; \\
			Update target networks \\
			$\theta^{\mu^-} \leftarrow \tau \theta^{\mu} + (1-\tau) \theta^{\mu^-}$ \\
			$\theta^{Q^-} \leftarrow \tau \theta^{Q} + (1-\tau) \theta^{Q^-}$;
		}
	}
	\caption{Deep Deterministic Policy Gradient}
	\label{alg:ddpg}
\end{algorithm}

\subsubsection{Twin Delayed Deep Deterministic Policy Gradient}
Twin Delayed Deep Deterministic Policy Gradient \cite{fujimoto_addressing_2018} is improved version of DDPG with higher stability and efficiency. There are three main tricks.

\textbf{Target Policy Smoothing}: For target value assesing, actions are obtained from target policy network in DDPG, while a clipped zero centered gaussian noise is added to actions in TD3 \ref{eqn:td3_target_action}. This regularizes learning process by smoothing effects of actions on value. 

\begin{equation}
\label{eqn:td3_target_action}
a'(s') = \mu(s';\theta^{\mu^-}) + \text{clip}(\epsilon, -c, c), \quad \epsilon \sim \mathcal{N}(0, \sigma)
\end{equation}

\textbf{Clipped Double Q Learning}: There are two different $Q$ networks with their targets. During learning, both of them are learned from single target value. This value is assessed by using whichever of two networks give smaller of it. This allows to escape overestimation of values. 

\begin{equation}
\label{eqn:td3_target}
Y_t^{TD3} = r_t + \gamma \min_{k\in\{1,2\}} Q(s_{t+1}, ;a'(s_{t+1});\theta^{Q_k^-})
\end{equation}

Policy is learned by maximizing output of first value network.

\begin{equation}
\label{eqn:td3_value_maximization}
\theta^\mu = \argmax \mathbb{E}_{s \sim U(\mathcal{D})} \Big[ Q(s, \mu(s_t;\theta^\mu);\theta^{Q_1}) \Big]
\end{equation}

\textbf{Delayed Policy Updates}: During learning, policy network and target networks are updated less frequently (at each fixed number of step) than value network. Since policy network parameters are learned by maximizing value network, it should be learned slower.

Algorithm is summarized in Algorithm \ref{alg:td3}. 

\begin{algorithm}[H]
	\SetAlgoLined
	\DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead
	Initialize: Replay memory $\mathcal{D}$ with capacity $N_{replay}$ \\
	Policy and action value function parameters $\theta^{\mu}$, $\theta^Q_1$, $\theta^Q_2$  \\
	Target policy and action value function parameters $\theta^{\mu^-} \leftarrow \theta^{\mu}$, $\theta^{Q^-}_1 \leftarrow \theta^{Q}_1$, $\theta^{Q^-}_2 \leftarrow \theta^{Q}_2$ \\
	Random process $\mathcal{N}$ as exploration noise \\
	Target policy smoothing parameters $\sigma$, $c$, Update delay parameter $d$ \\
	\For{$\text{episode} = 1, M $}{
		Recieve initial state $s_1$; \\
		\For{$t = 1, T$}{
			Select action $a_t = \mu(s_t; \theta^{\mu}) + \epsilon$ where $\epsilon \sim \mathcal{N}$
			Execute action $a_t$ and recieve reward $r_t$ and next state $s_{t+1}$; \\
			Store experience tuple $e_t = (s_t, a_t, r_t, s_{t+1})$ to $\mathcal{D}$ ; \\
			Sample random batch with $N$ transitions from $\mathcal{D}$ as $\mathcal{D}_{r}$; \\
			Evaluate target actions for value target
			$a'(s_{j+1}) = \mu(s_{j+1};\theta^{\mu^-}) + \text{clip}(\epsilon, -c, c), \quad \epsilon \sim \mathcal{N}(0, \sigma) \quad \forall e_j \in \mathcal{D}_{r}$; \\
			$\forall k \in \{1,2\}$, set $Y_{jk}^{TD3} = \begin{cases}
			r_j + \gamma Q(s_{j+1},a'(s_{j+1}); \theta^{Q^-}_k) & \text{if } s_{j+1} \text{ not terminal } \\
			r_j & \text{if } s_{j+1} \text{ terminal }
			\end{cases}
			\forall e_j \in \mathcal{D}_{r} $; \\
			Set $Y_j^{TD3} = \min(Y_{j1}^{TD3}, Y_{j2}^{TD3})$; \\
			Update $\theta^Q_1$ $\theta^Q_2$ by seperately minimizing $ \frac{1}{N}\sum_{e_j \in \mathcal{D}_{r}} \big( Y_j^{TD3} - Q(s_j,a_j;\theta^Q_k) \big) ^ 2 \quad \forall k \in \{1,2\}$ with a single optimization step; \\
			\uIf{$t \mod d$}{
				Update $\theta^{\mu}$ by maximizing $ \frac{1}{N}\sum_{e_j \in \mathcal{D}_{r}} Q(s_j,a_j;\theta^Q_1)$ with a single optimization step; \\
				Update target networks \\
				$\theta^{\mu^-} \leftarrow \tau \theta^{\mu} + (1-\tau) \theta^{\mu^-}$ \\
				$\theta^{Q^-}_k \leftarrow \tau \theta^{Q}_k + (1-\tau) \theta^{Q^-}_k \quad \forall k \in \{1,2\}$ ;
			}
		}
	}
	\caption{Twin Delayed Deep Deterministic Policy Gradient}
	\label{alg:td3}
\end{algorithm}