\section{Policy and Control}
\label{sec:policy_control}

\subsection{Policy}

Policy is logic of how agent acts according to state of environment. 
It can be either deterministic or stochastic. 

\begin{description}
	\item[Deterministic Policy $\mu \colon \mathcal{S} \mapsto \mathcal{A}$] 
	Mapping from states to actions.
	\item[Stochastic Policy $\pi \colon \mathcal{S} \times \mathcal{A} \mapsto \lbrack 0,1 \rbrack$] 
	Mapping from state-action pair to a probability value.
\end{description}

\subsection{Return}

At time $t$, Return $G_t$ is cumulative sum of future rewards scaled by discount factor $\gamma$. 
It is mathematically defined by 
\begin{equation}
G_t = \sum_{i=t}^{\infty} \gamma^{i-t} r_i = r_t + \gamma G_{t+1}.
\end{equation}
Since return depends on future rewards, it also depends on policy of agent since policy affects future rewards.

\subsection{State Value Function}

State Value Function $V^{\pi}$ is expected return when policy $\pi$ is followed in future and defined by
\begin{equation}
V^{\pi}(s) = \mathbb{E}[G_t|s_t=s, \pi]. % \quad \forall t = 0,1, ...
\end{equation}
Optimal value function should return maximum expected return. 
The behavior is controlled by policy. In other words, 
\begin{equation}
V^{*}(s) = \max_{\pi} V^{\pi}(s).
\end{equation}

\subsection{State-Action Value Function}

State-Action Value Function $Q^{\pi}$ is expected return when policy $\pi$ is followed in future, 
but any action taken at instant step. It is defined by
\begin{equation}
Q^{\pi}(s,a) = \mathbb{E}[G_t|s_t=s, a_t=a, \pi]. %\quad \forall t = 0,1, ...
\end{equation}
Optimal state-action value function ($Q^{*}$) should yield maximum expected return for each state-action pair. 
Mathematically,  
\begin{equation}
Q^{*}(s,a) = \max_{\pi} Q^{\pi}(s,a).
\end{equation}
Similarly, the optimal policy $\pi^*$ can be obtained by $Q^{*}(s,a)$. For stochastic policy, it is defined as follows.
\begin{equation}
\label{eqn:policy_stochastic_q}
\pi^{*}(s,a) = 
\begin{cases}
1,   & \text{if  } a = \argmax_{a} Q^{*}(s,a) \\
0,   & \text{otherwise  }
\end{cases} 
\end{equation}
For deterministic policy, 
\begin{equation}
\label{eqn:policy_deterministic_q}
\mu^{*}(s) = \argmax_{a} Q^{*}(s,a)
\end{equation}

\subsection{Bellman Equation}

Bellman proved that optimal value function should satisfy following conditions~\cite{bellman_dynamic_2003}. 
\begin{equation}
\label{eqn:bellman_v}
V^{*}(s) = \max_{a} \big( R(s,a) + \gamma \sum_{s'} T(s',s,a) V^{*}(s') \big)
\end{equation}
\begin{equation}
\label{eqn:bellman_q}
Q^{*}(s,a) = R(s,a) + \gamma \max_{a'} \big( \sum_{s'} T(s',s,a) Q^{*}(s',a') \big)
\end{equation}
