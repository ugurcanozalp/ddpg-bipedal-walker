\section{Policy and Control}
\label{sec:policy_control}

\subsection{Policy}

A policy defines how the agent acts according to the state of the environment. 
It may be either deterministic or stochastic: 

\begin{description}
	\item[Deterministic Policy $\mu \colon \mathcal{S} \rightarrow \mathcal{A}$]. 
	A mapping from states to actions.
	\item[Stochastic Policy $\pi \colon \mathcal{S} \times \mathcal{A} \rightarrow \lbrack 0,1 \rbrack$]. 
	A mapping from state-action pair to a probability value.
\end{description}

\subsection{Return}

At time $t$, return $G_t$ is a cumulative sum of the future rewards scaled by the discount factor $\gamma$: 
\begin{equation}
\label{eqn:return_dfn}
G_t = \sum_{i=t}^{\infty} \gamma^{i-t} r_i = r_t + \gamma G_{t+1}.
\end{equation}
Since the return depends on future rewards, it also depends on the  policy of the agent as it affects future rewards.

\subsection{State Value Function}

State Value Function $V^{\pi}$ is the expected return when policy $\pi$ is followed in future and it is defined by
\begin{equation}
V^{\pi}(s) = \mathbb{E}[G_t|s_t=s, \pi]. % \quad \forall t = 0,1, ...
\end{equation}
Optimal value function should return maximum expected return. 
The behavior is controlled by the policy. In other words, 
\begin{equation}
V^{*}(s) = \max_{\pi} V^{\pi}(s).
\end{equation}

\subsection{State-Action Value Function}

State-Action Value Function $Q^{\pi}$ is again the expected return when policy $\pi$ is followed in future, 
however, any action taken at the instant step:
\begin{equation}
Q^{\pi}(s,a) = \mathbb{E}[G_t|s_t=s, a_t=a, \pi]. %\quad \forall t = 0,1, ...
\end{equation}
Optimal state-action value function should yield maximum expected return for each state-action pair. Hence,
\begin{equation}
Q^{*}(s,a) = \max_{\pi} Q^{\pi}(s,a).
\end{equation}
Similarly, the optimal policy $\pi^*$ can be obtained by $Q^{*}(s,a)$. For stochastic policy, it is defined as follows: 
\begin{equation}
\label{eqn:policy_stochastic_q}
\pi^{*}(a|s) = 
\begin{cases}
1,   & \text{if  } a = \arg\max_{a} Q^{*}(s,a) \\
0,   & \text{otherwise  }
\end{cases} 
\end{equation}
For deterministic policy, 
\begin{equation}
\label{eqn:policy_deterministic_q}
\mu^{*}(s) = \arg\max_{a} Q^{*}(s,a)
\end{equation}

\subsection{Bellman Equation}

Bellman proved that optimal value function should satisfy following conditions~\cite{bellman_dynamic_2003}. 
\begin{equation}
\label{eqn:bellman_v}
V^{*}(s) = \max_{a} \Big\{ R(s,a) + \gamma \sum_{s'} T(s'|s,a) V^{*}(s') \Big\}
\end{equation}
\begin{equation}
\label{eqn:bellman_q}
Q^{*}(s,a) = R(s,a) + \gamma \max_{a'} \Big\{ \sum_{s'} T(s'|s,a) Q^{*}(s',a') \Big\}
\end{equation}
