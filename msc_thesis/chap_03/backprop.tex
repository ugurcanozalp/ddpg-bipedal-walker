\section{Backpropagation and Numerical Optimization}
Neural networks are composed of weight parameters. Learning is process of updating weights to give desired behavior. This is represented in a loss function. Learning is nothing but minimizing it by numerical optimization methods. \\

In order to minimize a loss function, its gradient with respect to weight parameters needs to be calculated. These gradients are computed by chain rule. Therefore, gradient information propagates backward, and this process is called backpropagation. 

\subsection{Stochastic Gradient Descent Optimization}

Gradient Descent minimizes loss function $\mathcal{L}$ by updating weight parameters $\theta$ to inverse of gradient direction with a learning rate $\eta$. 

\begin{equation}
\label{eq: grad_desc}
\theta \leftarrow \theta - \eta \nabla \mathcal{L}(\theta)
\end{equation}

In machine learning problems, loss functions have usually summed form of sample losses \eqref{eqn:summed_loss}. Stochastic Gradient Descent approximate gradient of loss function by sample losses and updates parameters accordingly \eqref{eqn:stch_grad_desc}. 

\begin{equation}
\label{eqn:summed_loss}
\mathcal{L}(\theta) = \frac{1}{N} \sum_{i=1}^{N} \mathcal{L}_i(\theta)
\end{equation}

\begin{equation}
\label{eqn:stch_grad_desc}
\theta \leftarrow \theta - \eta \nabla \mathcal{L}_i(\theta) \forall i \in {1,N}
\end{equation}

\subsection{Adam Optimization}

Adam \cite{kingma_adam_2017}  (short for Adaptive Moment Estimation) is a variant of stocastic gradient descent as combination of RMSProp \cite{hinton_lecture_nodate} and AdaGrad \cite{duchi_adaptive_2011} algorithms. It is one of mostly used optimization method in deep learning nowadays. Adam adjusts learning rate based on training data to overcome issues arised due to stochastic updates to accelerate training and make it robust to learning rate. \\





