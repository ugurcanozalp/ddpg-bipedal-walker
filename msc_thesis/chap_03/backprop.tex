\section{Backpropagation and Numerical Optimization}
\label{sec:backprop}

Neural networks are composed of weight parameters. 
Learning is the process of updating weights to give desired behavior. 
This behavior is represented in a loss function. Thus, learning is nothing but minimization of loss by numerical optimization methods. 

In order to minimize a loss function, its gradient with respect to weight parameters needs to be calculated. 
These gradients are obtained by chain rule of basic calculus. 
Therefore, gradient information propagates backward, and this process is called backpropagation. 

\subsection{Stochastic Gradient Descent Optimization}

Gradient descent minimizes the loss function $\mathcal{L}$ by updating the weight parameters, say, $\theta$ to opposide of gradient direction with a predefined learning rate $\eta$, 
\begin{equation}
\label{eq: grad_desc}
\theta \leftarrow \theta - \eta \nabla \mathcal{L}(\theta).
\end{equation}

In machine learning problems, loss functions have usually summed form of sample losses $\mathcal{L}_i$: 
\begin{equation}
\label{eqn:summed_loss}
\mathcal{L}(\theta) = \frac{1}{N} \sum_{i=1}^{N} \mathcal{L}_i(\theta).
\end{equation}
Stochastic Gradient Descent approximate gradient of loss function by sample losses and updates parameters accordingly,
\begin{equation}
\label{eqn:stch_grad_desc}
\theta \leftarrow \theta - \eta \nabla \mathcal{L}_i(\theta) \quad \forall i \in \{1,2, \cdots N\}.
\end{equation}

However, in practice, mini-batches are used to estimate loss gradient. 
In that case, batches with size $N_b$ are sampled from instances, \begin{equation}
\label{eqn:mb_summed_loss}
\mathcal{L}_j(\theta) = \frac{1}{N_b} \sum_{i=1 + (j-1) N_b}^{j N_b} \mathcal{L}_i(\theta),
\end{equation}
and updates are performed accordingly,
\begin{equation}
\label{eqn:mb_grad_desc}
\theta \leftarrow \theta - \eta  \nabla \mathcal{L}_j(\theta) \quad \forall j \in \{1,2, \cdots \Big\lfloor\frac{N}{N_b}\Big\rfloor\}.
\end{equation}

\subsection{Adam Optimization}

Adam~\cite{kingma_adam_2017}  (short for Adaptive Moment Estimation) is a variant of stochastic gradient descent as improvement of RMSProp~ \cite{hinton_lecture_nodate} algorithm. 
It scales the learning rate using second moment of gradients as in RMSprop and uses momentum estimation for both first and second moment of gradients. 

It is one of mostly used optimization method in deep learning nowadays. 
Adam adjusts step length based on training data to overcome issues arised due to stochastic updates in order to accelerate training and make it robust. 
It is summarized in Algortihm~\ref{alg:adam}. Note that $\odot$ and $\oslash$ are elementwise multiplication and division respectively. 

\begin{algorithm}[H]
	\SetAlgoLined
	\DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead
	Initialize: Learning Rate $\eta$, Moving average parameters $\beta_1$, $\beta_2$\\
	Initial Model parameters $\theta_0$ \\
	Initial first and second moment of gradients $m \leftarrow 0$, $v \leftarrow 0$ \\
	Initial step $j \leftarrow 0$ \\
	\While{$\theta_j$ \text{not converged}}{
		$j \leftarrow j+1$ \\
		$g_j \leftarrow \nabla \mathcal{L}_j(\theta)$ (Obtain gradient) \\
		$m_j \leftarrow \beta_1 m_{j-1} + (1-\beta_1) g_j$ (Update first moment estimate) \\
		$v_j \leftarrow \beta_2 v_{j-1} + (1-\beta_2) g_j \odot g_j$ (Update second moment estimate) \\
		$\hat{m}_j \leftarrow \frac{m_j}{1-\beta_1^j}$ (First moment bias correction) \\
		$\hat{v}_j \leftarrow \frac{v_j}{1-\beta_2^j}$ (Second moment bias correction) \\
		$\theta_j \leftarrow \theta_{j-1} - \eta \hat{m}_j \oslash (\hat{v}_j + \epsilon)$ (Update parameters) \\
	}
	\caption{Adam Optimization Algorithm}
	\label{alg:adam}
\end{algorithm}
