\section{Neural Networks}
\label{sec:nnet}
The first models of neural network developed by a neurophysiologist Warren McCulloch and a mathematician Walter Pitts in 1943 \cite{mcculloch_logical_1943}. However, the idea of neural network known today arised after development of a simple binary classifier called perceptron invented by Rosenblast et al. \cite{rosenblatt_perceptron_1958}. It is a learning framework inspired by human brain.
Although there are many types of neural networks, they are all based on linear transformations followed by nonlinear activations.
Unlike the SVM it produces probabilistic outputs, although this
is at the expense of a nonconvex optimization during training.

\subsection{Perceptron}
Perceptron is a binary classifier model. In order to allocate input $\boldsymbol{x}$ into a class, feature vector $\boldsymbol{\phi}(\boldsymbol{x})$ is generated by a fixed nonlinear function. Then, a linear model is generated with linear transformation weights $\boldsymbol{w}$ in the following form \ref{eqn:perceptron1}.

\begin{equation}
\label{eqn:perceptron1}
y = f(\boldsymbol{w}^\intercal \boldsymbol{\phi}(\boldsymbol{x}))
\end{equation}

where $f$ is called activation function. For perceptron, it is defined as step function \ref{eqn:stepfun} while other functions like sigmoid, tanh can also be defined.

\begin{equation}
\label{eqn:stepfun}
f(a) = 
\begin{cases}
1,   & \text{if } a\geq 0\\
0,   & \text{otherwise}
\end{cases} 
\end{equation}

A learning algorithm of a perceptron aims determining the parameter vector $\boldsymbol{w}$. It is best motivated by error minimization of data samples once a cost function is constructed.

\subsection{Perceptron as Neural Layer}
Structure of perceptron make a way for feed forward neural layers. Unlike stated below, a neural layer might output multiple values as vector. Such a setting forces parameter $\boldsymbol{w}$ to be a matrix. Moreover, activation function is not necessariliy step function. It can be any nonlinear function like sigmoid, tanh, relu etc. 

\subsection{Feed Forward Neural Networks (Multilayer Perceptron)}
Feed Forward Neural Networks are generalization of perceptron algorithm to approximate any function $f^*$. Neural layers are stacked to construct deep feed forward neural network. It defines a nonlinear mapping $y=f(\boldsymbol{x};\boldsymbol{\theta})$ between input $\boldsymbol{x}$ and output $y$, parametrized by parameters $\boldsymbol{\theta} = \{\boldsymbol{w}\}_n,n=1,â€¦,N.$.

Assuming input signal is $\boldsymbol{x}$ (output of previous layer), activation value of the layer ($\boldsymbol{h}$) is evaluated as by linear transformation followed by nonlinear activation $f$ \ref{eqn:mlpact} applied elementwise.
\begin{equation}
\label{eqn:mlpact}
\boldsymbol{net} = \boldsymbol{W} \boldsymbol{x} + \boldsymbol{b} \text{ and } \boldsymbol{h} = f(\boldsymbol{net})
\end{equation}

\subsection{Convolutional Neural Networks}
Convolutional Neural Networks (CNNs) are special kind of neural network to process data which has grid-like topology. Such data can be time series (1D) image (2D), video (3D) etc. CNNs are simple neural network with at least one convolutional layer. Convolutional layers implements an operation called convolution, which is a linear operator. Convolution is followed by activation and pooling. Pooling is reduction of activation values in order to extract important features from grid space.

\subsubsection{Convolution Operation}
Similar to definition in signal processing terminolgy, convolution is smoothing input tensor with a predefined kernel. However, the kernel is trainable parameter for CNNs. Let $\boldsymbol{x}$ be $n$ dimensional signal and $\boldsymbol{w}$ is the kernel. 1D convolution \ref{eqn:conv1d} and 2D convolution \ref{eqn:conv2d} of $\boldsymbol{x}$ by kernel $\boldsymbol{w}$, let it be $\boldsymbol{s}$, is the following.

\begin{equation}
\label{eqn:conv1d}
s_i = (x*w)_i = \sum_{m} x_{i+m} w_m
\end{equation}

\begin{equation}
\label{eqn:conv2d}
s_{ij} = (x*w)_{ij} = \sum_{m} \sum_{n} x_{i+m,j+n} w_{mn}
\end{equation}

Convolution preserves the spatial relationship of input by learning local features using small spans of input data. 

\subsubsection{Nonlinear Activation}
After convolution operation, usually nonlinear activation is applied on output of convolution operation.

\subsubsection{Pooling}
A typical layer of a convolutional network consists of three stages; convolution, activation and pooling. Activation is mapping features by nonlinear function as stated earlier, it is kind of detecting important features of grid points. The last step is pooling which is for grabbing meaningful features and leave others.
Pooling makes input representation invariant to translation. This enables layer to extract required features whereever they are in grid space. This also downsamples input and makes computations easier.


\subsection{Recurrent Neural Networks}
Recurrent Neural Networks (RNNs) \cite{rumelhart_learning_1986} are type of neural network to process sequential data. Similar to CNNs, it is specialized for data having grid-like topology. It is used commonly used for sequence based applications.
Sequential data can be inferred by Recurrent Neural Networks. In Feed Forward Layers, output only depends on its input, while Recurrent Layer output is dependent on both input at time $t$ and its output in previous time step $t-1$.

RNN can be thought as multiple copies of same network which passes message to its successor through time. A RNN layer is similar to MLP layer \ref{eqn:mlpact}, except input is concatenation of output feedback and input itself \ref{eqn:rnnact}.

Assume previous layer output and input of layer at time $t$ is $\boldsymbol{x}(t)$. Activated value of layer is linear function of $\boldsymbol{x}(t)$ and output from previous time step, $\boldsymbol{h}(t-1)$. Again, nonlinear activation \ref{eqn:mlpact} applied elementwise. 

\begin{equation}
\label{eqn:rnnact}
\boldsymbol{net}(t) = \boldsymbol{\tilde{W}} \boldsymbol{h}(t-1) + \boldsymbol{W} \boldsymbol{x}(t) + \boldsymbol{b} \text{ and } \boldsymbol{h}(t) = f(\boldsymbol{net}(t))
\end{equation}


\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{figures/ml_theory/rnn_vs_ffnn_layer.png}
	\caption{Recurrent Layer (left) and Feed Forward Layer (right) illustration.}
	\label{fig:rnn_vs_ffnn}
\end{figure}

\subsubsection{Long Term Dependence Problem of Vanilla RNNs}
Conventional RNNs have problem with vanishing/exploding gradient problem. This causes long term dependence problem. For example, given word sequence, bold words in the following sentences are hard to predict with Vanilla RNN.

\centerline{The clouds are in the \textbf{sky}.}
\centerline{I grew up in France... I speak fluent \textbf{French}.}

In order to overcome this problem new architectures are developed such as Long Short Term Memory (LSTM) and Gated Recurrent Units (GRU).

\subsubsection{Long Short Term Memory}
LSTM is a special type of RNN. It is explicitly designed to allow learning long-term dependencies. A single LSTM cell has 4 neural layer while vanilla RNN layer has only one neural layer. In addition to hidden state $\boldsymbol{h}(t)$ , there is another state called cell state $\boldsymbol{C}(t)$. Information flow is controlled by 3 gates. 

\begin{figure}
	\centering
	\includegraphics[width=0.95\textwidth]{figures/ml_theory/lstm/lstm_module.png}
	\caption{LSTM Cell.}
	\label{fig:lstm_cell}
\end{figure}

\textbf{Forget Gate}: Forget gate controls past memory. According to input, past memory is either kept or forgotten. Sigmoid function ($\sigma$) is used as activation function, applied elementwise.

\begin{equation}
\label{eqn:lstm_forget}
\boldsymbol{f}(t) = \sigma(\boldsymbol{W}_{f} [\boldsymbol{h}(t-1); \boldsymbol{x}(t)] + \boldsymbol{b}_{f}) 
\end{equation}

\textbf{Input Gate}: Input gate controls contribution from input to cell state (memory). Hyperbolic tangent layer creates new candidate of cell state from input.

\begin{equation}
\label{eqn:lstm_inp}
\boldsymbol{i}(t) = \sigma(\boldsymbol{W}_{i} [\boldsymbol{h}(t-1); \boldsymbol{x}(t)] + \boldsymbol{b}_{i}) 
\end{equation}

\begin{equation}
\label{eqn:lstm_cellstcand}
\boldsymbol{\hat{C}}(t) = \tanh(\boldsymbol{W}_{C} [\boldsymbol{h}(t-1); \boldsymbol{x}(t)] + \boldsymbol{b}_{C}) 
\end{equation}

\textbf{Cell State Update}: Once what are to be forget and added are decided, cell state is updated.

\begin{equation}
\label{eqn:lstm_cellstupt}
\boldsymbol{C}(t) = \boldsymbol{f}(t) \odot \boldsymbol{C}(t-1) + \boldsymbol{i}(t) \odot \boldsymbol{\hat{C}}(t)
\end{equation}


\textbf{Output Gate}: Sigmoid layer decides what part of new cell state to be output. Cell state is filtered by tanh to push values to be in $(-1,1)$.

\begin{equation}
\label{eqn:lstm_out}
\boldsymbol{o}(t) = \sigma(\boldsymbol{W}_{o} [\boldsymbol{h}(t-1); \boldsymbol{x}(t)] + \boldsymbol{b}_{o}) 
\end{equation}

\begin{equation}
\boldsymbol{h}(t) = \boldsymbol{o}(t) \odot \tanh(\boldsymbol{C}(t))
\end{equation}

\subsection{Attention Mechanism}
As stated earlier, recurrent neural networks are prone to forget long term dependencies. LSTM and GRU are invented to overcome this problem. Although they reduced this problem, they cannot attend specific parts of the input. For example, for sentiment analysis, specific keywords are important to determine sentiment of a sentence. However, last state of encoded input is not able to remember that words. Therefore, people came with the idea of weighted avearing all states through time where weights depends on both input and output. 

Assume that input sequence $X \in \mathbb{R}^{T \times d_X}$ is encoded to $H \in \mathbb{R}^{T \times d_H}$. The context vector is calculated using weight vector $\alpha(t)$. 

Calculation of weight vector depends on the task. For each time step, a score function is calculated between hidden state $H \in \mathbb{R}^{T \times d_H}$ and query $\boldsymbol{q}$ (which may be many things depending on task). Also, score function is also depends on choice. Then, attention score is $\boldsymbol{\alpha} \in \mathbb{R}^{T}$ is calculated using arbitrary function $f$ depending on choice.

\begin{equation}
\begin{split}
\boldsymbol{\alpha} = & f(\boldsymbol{q}, H) \\
\mathrm{Attention}(q, H) = & \sum_{\tau=0}^{T} \alpha_{\tau} h_{\tau}
\end{split}
\end{equation}


\subsubsection{Transformer}
The Transformer was proposed in the paper Attention is All You Need \cite{vaswani_attention_2017} . Unlike recurrent networks, this architecture is solely builded on attention layers. 

A transformer layer consists of feed-forward and attention layers, which makes the mechanism special. Like RNNs, it can be used as both encoder and decoder. While encoder layers attend to itself, decoder layers attends both itself and encoded input.

\textbf{Attention Layer}: An attention layer is a mapping from 3 vectors called query $Q \in \mathbb{R}^{T \times d_k}$, key $K \in \mathbb{R}^{T \times d_k}$ and value $V \in \mathbb{R}^{T \times d_v}$ to output, where $T$ is time length, $d_k$ and $d_v$ are embedding dimensions. Output is weighted sum of values $V$ while weights are evaluated by compatibility metric of query $Q$ and key $K$. In vanilla transformer, compatibility of query and key is evaluated by dot product, normalizing by $sqrt(d_k)$. For a query, dot product with all keys are evaluated, then softmax function is applied to get weights of values. This approach is called Scaled Dot-product Attention.

\begin{equation}
\mathrm{Attention}(Q, K, V) = \mathrm{softmax}(\frac{Q^{T} K}{\sqrt{d_k}}) V
\end{equation}

\textbf{Multi-Head Attention}: Instead of performing single attention; keys, queries and values are linearly projected from $d_m$ dimensional vector space to $h$ different spaces using projection matrices. Then, attention is done $h$ times, and results are then concatenated and linearly projected to final values of the layer.

Projection matrices are model parameters, $W^Q_i \in \mathbb{R}^{d_m \times d_k}$, $W^K_i \in \mathbb{R}^{d_m \times d_k}$, $W^V_i \in \mathbb{R}^{d_m \times d_v}$ for $i=1,...,h$. Also output matrix is used to project multiple values into single one, $W^O \in \mathbb{R}^{h d_v \times d_m}$.

\begin{equation}
\begin{split}
\mathrm{MHA}(Q,K,V)= & \mathrm{Concat}(a_1, a_2, ... a_h)W^O \\
a_i = & \mathrm{Attention}(QW^Q_i,KW^K_i,VW^V_i)
\end{split}
\end{equation}

\textbf{Feed Forward Layer}: Both encoder and decoder contains feed forward layer, containing two linear transformations with ReLU activation.

\begin{equation}
\mathrm{FFN}(x) = \mathrm{ReLU}(xW_1+b)W_2+b_2
\end{equation}

\textbf{Layer Normalization}: Layer normalization is a layer to overcome xyz []. Given an input $x \in \mathbb{R}^K$, mean and variance statistics are evaluated along the dimensions \eqref{eq:layernorm_statistics}.

\begin{equation}
\label{eq:layernorm_statistics}
\begin{split}
\mu = & \frac{1}{K} \sum_{n=1}^{K} x_k \\
\sigma^2 = & \frac{1}{K} \sum_{n=1}^{K} (x_k-\mu)^2
\end{split}
\end{equation} 

Then, the input is first scaled to have zero mean and unity variance along dimensions. The term $\epsilon$ is added to prevent division by zero. Optionally, the result is scaled by elementwise multiplication by $\gamma \in \mathbb{R}^K$ and addition by $\beta \in \mathbb{R}^K$ where these are learnable parameters. 

\begin{equation}
\label{eqn:layernorm}
\mathrm{LayerNorm}(x) = \frac{x-\mu}{\sigma+\epsilon} * \gamma + \beta
\end{equation}

\textbf{Encoder Layer}: Encoder Layer starts with a residual self attention layer. Self attention means that query, key and value are same vectors. Then it is followed by feed forward neural layer, Both sublayers are employed with resudial connection with layer normalization, i.e summation of layer input and output is passed through layer normalization.

\begin{equation}
\begin{split}
att = & \mathrm{LayerNorm}(x+ \mathrm{MHA}(x,x,x)) \\
y = & \mathrm{LayerNorm}(att+ \mathrm{FFN}(att))
\end{split}
\end{equation}

\textbf{Decoder Layer}: Similar to encoder layer, decoder layer has also self-attention and feed forward layers. In addition, there is another attention layer which is over encoder outputs. Same as encoder, all sublayers have resudial connection with layer normalization. Let's call encoded sequence $e \in \mathbb{R}^{T \times d_m}$ and decoded sequence $d \in \mathbb{R}^{T \times d_m}$ (masked). Assume that the sequence decoded up to $t$th point in sequence. Then, $d_{t+1}$ is calculated as follows.

\begin{equation}
\begin{split}
att = & \mathrm{LayerNorm}(d+ \mathrm{MHA}(d_{1:t},d_{1:t},d_{1:t})) \\
dec = & \mathrm{LayerNorm}(att+ \mathrm{MHA}(att,e,e)) \\
d_{t+1} = & \mathrm{LayerNorm}(dec+ \mathrm{FFN}(dec))
\end{split}
\end{equation}

\textbf{Positional encoding}: Since there are no recurrent or convolutional architecture in the model, sequential information needs to be embedded. Positional encodings has same dimension, so that input embeddings can be added to at the beginning of encoder or decoder stacks. For position $pos$, $2i$ or $2i+1$th dimension has following values ($i \in \mathbb{N}$), as proposed in the original paper.

\begin{equation}
\begin{split}
\mathrm{PE}_{pos,2i} = \sin(pos/10000^{2i/d_m}) \\
\mathrm{PE}_{pos,2i+1} = \cos(pos/10000^{2i/d_m})
\end{split}
\end{equation}

\subsubsection{Pre-Layer Normalized Transformer}
This and that...