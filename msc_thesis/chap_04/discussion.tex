\section{Discussion}
We believe that these results are not enough to say any model is superior to another, because there are other factors such as DRL method, number of episodes, network size etc. 
However, networks are designed to have similar sizes and good model requires to converge in less episodes. 
As a result, it is possible to say that transformers are able to surpass performance of LSTMs for partially observed RL problems. 
Note that this is valid where layer normalization is applied before multihead attention and feed-forward layers \cite{xiong_layer_2020} as opposed to vanilla transformer proposed in \cite{vaswani_attention_2017}. 

Another result is that incorprating past observations did not improve performance significantly, although the opposite was expected. 
We can interpret that either the history length is less or the partial observability is not a big deal for the agent. 

The environment is a difficult one. 
There are really few available models which gets close to a solution. 
Apart from neural networks, there are other factors affecting performance such as RL algorithm, rewarding, exploration etc. 
In this work, all of them are manipulated such that the environment becomes solvable. 
Time frequency is reduced for sample efficiency and speed. 
Also, the agent is not informed for terminal state when it reaches time limit, because it is logically wrong. Those modifications seems to be behind getting close to a solution. 
Lastly, Punishment of failing reduced, so the agent is allowed to learn by mistakes. 

As RL algorithm, TD3 is selected first, since it is a good choice for continuous RL. 
Ornstein-Uhlenbeck noise is used for better exploration since it has momentum, and its variance reduced by time to make agent learn small actions well in late episodes. 
In addition, SAC is used for learning along with TD3. Results seem better compared to TD3. 
SAC policy maximizes randomness (entropy) in policy if agent cannot get sufficient reward and this allows to decide where/when to explore more and less. 
The environment has sparse rewards, so SAC might be handled this problem better compared to TD3 thanks to this reason.

