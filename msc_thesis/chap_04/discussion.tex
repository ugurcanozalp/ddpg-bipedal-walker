\section{Discussion}
We believe that these results are not enough to conclude on a superior neural network, because there are other factors such as DRL algorithm, number of episodes, network size etc. 
However, networks are designed to have similar sizes and good model requires to converge in less episodes. 
As a result, it is possible to conclude that transformers are able to surpass performance of LSTMs for partially observed RL problems. 
Note that this is valid where layer normalization is applied before multihead attention and feed-forward layers \cite{xiong_layer_2020} as opposed to vanilla transformer proposed in \cite{vaswani_attention_2017}. 

Another result is that incorprating past observations did not improve performance significantly, although the opposite was expected. 
We can interpret that either the history length is less or the partial observability is not a big deal for the agent. 

The environment is a difficult one. 
There are really few available models which gets close to a solution. 
Apart from neural networks, there are other factors affecting performance such as RL algorithm, rewarding, exploration etc. 
In this work, all of them are adjusted such that the environment becomes solvable. 
Time frequency is reduced for sample efficiency and speed. 
Also, the agent is not informed for the terminal state when it reaches time limit. 
Those modifications seems to be behind getting close to a solution. 
Lastly, punishment of failing reduced, so the agent is allowed to learn by mistakes. 
Since neural design does not seem to affect performance significantly, such modifications are probably source of our high performance.

As RL algorithm, TD3 is selected first, since it is a good choice for continuous RL. 
Ornstein-Uhlenbeck noise is used for better exploration since it has momentum, and its variance is reduced by time to make agent learn small actions well in later episodes. 
In addition, SAC is used for learning along with TD3. Results seem better compared to those of TD3. 
SAC policy maximizes randomness (entropy) if agent cannot get sufficient reward and this allows the agent to decide where/when to explore more or less. 
This way, SAC handles the sparse rewards from the environment better than TD3. 
