\section{Discussion}
We believe that these results are not enough to say any model is superior to another, because there are other factors such as DRL method, number of episodes, network size etc. 
However, networks are designed to have similar sizes and good model requires to converge in less episodes. 
As a result, it is possible to say that transformers are able to surpass performance of LSTMs for partially observed RL problems. 
Note that this is valid where layer normalization is applied before multihead attention and feed-forward layers \cite{xiong_layer_2020} as opposed to vanilla transformer proposed in \cite{vaswani_attention_2017}. 

Another result is that incorprating past observations did not improve performance drastically, although the opposite was expected. 
We can interpret that either the history length is less or the partial observability is not a big deal for the agent. 

The environment is a difficult one. 
There are really few available models which gets close to a solution. 
Apart from neural networks, there are other factors affecting performance such as RL algorithm, rewarding, exploration etc. 
In this work, all of them are manipulated such that the environment becomes solvable. 
TD3 algorithm is selected since it is a good choice for continious RL. 
Punishment of failing reduced, so the agent is allowed to learn by mistakes. 
Ornstein-Uhlenbeck noise is used for better exploration since it has momentum, and its variance reduced by time to make agent learn small actions well in late episodes. 
Time frequency is reduced for sample efficiency and speed. 
Lastly, the agent is not informed for terminal state when it reaches time limit, because it is logically wrong. Those modifications seems to be behind getting close to a solution. 
