\chapter{BIPEDAL WALKING BY TWIN DELAYED DEEP DETERMINISTIC POLICY GRADIENTS}
\label{chap:exp_setup}

\section{Details of the Environment}

\textit{Bipedal-Walker-v3} and \textit{Bipedal-Walker-Hardocore-v3} are simulation environments of a bipedal robot, with relatively flat course and obstacle course respectively. Dynamics of the robot are exactly identical in both environments. Components of the hardcore environment is visualized in \figref{fig:bipedal_hardcore_components}.

\begin{figure}
	\centering
	\includegraphics[width=0.95\textwidth]{figures/bipedal/bpedal_annotated.png}
	\caption{Bipedal Walker Hardcore Components}
	\label{fig:bipedal_hardcore_components}
\end{figure}

The robot has kinematic and lidar sensors. It is modeled as Markov Decision Process with deterministic dynamics. 

\textbf{Observation Space}: Hull angle, hull angular velocity, translational velocity on two dimension, joint positions, joint angular speeds, leg ground concats and 10 lidar rangefinder measurements. Details are summarized at Table \ref{table:bpw_obs_space}

\begin{table}[h!]
	\begin{center}
	\begin{tabular}{cccc}
		\textbf{Num} & \textbf{Observation} & \textbf{Interval} \\
		\hline 
		0  & Hull Angle & $[-\pi,\pi]$ \\
		1  & Hull Angular Speed & $[-\infty,\infty]$ \\
		2  & Velocity x & $[-1,1]$ \\
		3  & Velocity y &$[-1,1]$ \\
		4  & Hip 1 Joint Angle & $[-\infty,\infty]$ \\
		5  & Hip 1 Joint Speed & $[-\infty,\infty]$ \\
		6  & Knee 1 Joint Angle & $[-\infty,\infty]$ \\
		7  & Knee 1 Joint Speed & $[-\infty,\infty]$ \\
		8  & Leg 1 Ground Contact Flag & $\{0,1\}$ \\
		9  & Hip 2 Joint Angle & $[-\infty,\infty]$ \\
		10  & Hip 2 Joint Speed & $[-\infty,\infty]$ \\
		11  & Knee 2 Joint Angle & $[-\infty,\infty]$ \\
		12  & Knee 2 Joint Speed & $[-\infty,\infty]$ \\
		13  & Leg 2 Ground Contact Flag & $\{0,1\}$ \\
		14-23  & Lidar measures  & $[-\infty,\infty]$
	\end{tabular}
	\end{center}
	\caption{Observation Space of Bipedal Walker}
	\label{table:bpw_obs_space}
\end{table}

\textbf{Action Space}: The robot has 2 legs with 2 joints at knee and hip. Torque provided to knee and pelvis joints of both legs. Details are presented in Table \ref{table:bpw_act_space}.

\begin{table}[h!]
	\begin{center}
		\begin{tabular}{cccc}
			\textbf{Num} & \textbf{Observation} & \textbf{Interval} \\
			\hline
			0  & Hip 1 Torque & $[-1,1]$ \\
			1  & Hip 2 Torque & $[-1,1]$ \\
			2  & Knee 1 Torque & $[-1,1]$ \\
			3  & Knee 2 Torque & $[-1,1]$ \\
		\end{tabular}
	\end{center}
	\caption{Action Space of Bipedal Walker}
	\label{table:bpw_act_space}
\end{table}

\textbf{Rewarding}: The robot should run fast with little energy while should not stumble and fall to ground. Therefore reward is shaped accordingly. Directly proportional to distance traveled forward, +300 points given if agent reaches end of path. -10 points (-100 points in original version)  if agent falls, and small amount of negative reward proportional to applied motor torque (preventing applying unnecessary torque). Lastly, the robots gets negative reward propotional to absolut value of hull angle for reinforcing to keep hull straigth. 

\textbf{Modifications on Original Envrionment}: In original version, agent gets -100 points when its hull hits the floor. In order to make the robot more greedy, this is changed to -10 points. In addition, time frequency of simulation is halved (from 50 Hz to 25 Hz) by only observing one of each two consecutive frames using a custom wrapper function. 

\subsection{Partial Observability}
In DRL, partial observability is handled by 2 ways in literature \cite{dulac-arnold_challenges_2019}. First is incorprating fixed number of last observations while second way is updating hidden belief state using recurrent neural network at each time step. Our approach is using fixed number of past observation into LSTM, BiLSTM and Transformer based networks for both actor and critic networks. 

\section{Proposed Neural Networks}

For all networks, varing backbones used to encode state information from observations for both actor and critic networks. In critic network, actions are concatenated by state information coming from backbones. Then, this concatenated vector is passed through feed forward layer with GELU activation then a linear layer with single output. In actor network, backbone is followed by a single layer with tanh activation for action estimation. As backbones, following networks are proposed.

\subsection{Feed Forward Network with residual connection}
Observations are passed through a layer with tanh activation to 96 dimensional output. Then it is passed through 2 layers with 192 dimensional hidden size and 96 dimensional output. This output is summed with first layer's output and this is lastly passed through layer normalization.

\subsection{Long Short Term Memory}
Sequence of observations are passed through a layer with tanh activation to 96 dimensional output. Then it is passed though vanilla lstm layer with 96 dimensional hidden state.

\subsection{Transformer (Pre-layer Normalized)}
Sequence of observations are passed through a layer with tanh activation to 96 dimensional output. Then it is passed though pre-layer normalized transformer with 192 dimensional feed forward layer. The output is lastly passed through layer normalization.


\section{RL Method and hyperparameters}
TD3 algorithm is used for learning task. Hyperparameters are selected by grid search and best performing values are used. Adam optimizer is used as optimizer. \\
As exploration noise, ornstein-uhlenbeck noise is used and standart deviation is multiplied  by $0.999$ at the end of each episode. Initially $\theta=4.0$ and $\sigma=1.0$ are used. \\
Other hyperparameters are as follows. \\
\begin{itemize}
	\item $\alpha=7.0 \cdot 10^{-4}$ (For LSTM)
	\item $\alpha=1.0 \cdot 10^{-3}$ (For Other networks)
	\item $\beta_1=0.9$
	\item $\beta_2=0.999$
	\item $\gamma=0.99$
	\item $N_{replay} = 500000$.
\end{itemize}

\section{Results}
