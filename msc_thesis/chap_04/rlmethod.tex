\section{RL Method and Hyperparameters}
\label{sec:rlmethod}

TD3 and SAC is used as RL algorithm. 
Hyperparameters are selected by grid search and best performing values are used. Adam optimizer is used for optimization. 
For all models, agents are trained by 8000 episodes.

In TD3, as exploration noise, Ornstein-Uhlenbeck noise is used, and standart deviation is multiplied  by $0.9995$ at the end of each episode. For LSTM and Transformer, last 6 observations are used to train agent. 
More than 6 observations made TD3 algorithm diverge. 

In SAC model, squashed gaussian policy is implemented. 
Therefore, along with layer giving mean actions, an additive layer is implemented for the standart deviation. 
For LSTM and Transformer, last 6 observations and 12 observations are used to train agent. 

All hyperparameters are found after a trial-error process considering the literature. They  are summarized in \tabref{table:hyperparams_td3} and \tabref{table:hyperparams_sac}. 
Training sessions are run by multiple times to make comparison fair among models since some training sessions fail to converge and some yield worse results.

\begin{table}
	\caption{Hyperparmeters and Exploration of Learning Processes for TD3}
	\begin{tabular}{|l||*{3}{c|}}\hline
		\backslashbox{Hyperparameter}{Model}
		&\makebox[5em]{RFFNN}&\makebox[5em]{LSTM}&\makebox[5em]{Transformer}\\\hline\hline
		$\eta$ (Learning Rate) & \multicolumn{3}{|c|}{$4.0\times10^{-4}$}\\\hline
		$\beta$ (Momentum) & \multicolumn{3}{|c|}{$(0.9, 0.999)$}\\\hline
		$\gamma$ (Discount Factor) & \multicolumn{3}{|c|}{$0.98$} \\\hline
		$N_{replay}$ (Replay Buffer Size) &\multicolumn{3}{|c|}{$500000$} \\\hline
		$N$ (Batch Size) &\multicolumn{3}{|c|}{$64$}\\\hline
		$d$ (Policy Delay) &\multicolumn{3}{|c|}{$2$}\\\hline
		$\sigma$ (Policy smoothing std) &\multicolumn{3}{|c|}{$0.2$}\\\hline
		$\tau$ (Polyak parameter) &\multicolumn{3}{|c|}{$0.01$}\\\hline
		Exploration &\multicolumn{3}{|c|}{$OU(\theta=4.0, \sigma=1.0)$}\\\hline
	\end{tabular}
	\label{table:hyperparams_td3}
\end{table}
\noindent

\begin{table}
	\caption{Hyperparmeters and Exploration of Learning Processes for SAC}
	\begin{tabular}{|l||*{3}{c|}}\hline
		\backslashbox{Hyperparameter}{Model}
		&\makebox[5em]{RFFNN}&\makebox[5em]{LSTM}&\makebox[5em]{Transformer}\\\hline\hline
		$\eta$ (Learning Rate) & \multicolumn{3}{|c|}{$4.0\times10^{-4}$}\\\hline
		$\beta$ (Momentum) & \multicolumn{3}{|c|}{$(0.9, 0.999)$}\\\hline
		$\gamma$ (Discount Factor) & \multicolumn{3}{|c|}{$0.98$} \\\hline
		$N_{replay}$ (Replay Buffer Size) &\multicolumn{3}{|c|}{$500000$} \\\hline
		$N$ (Batch Size) &\multicolumn{3}{|c|}{$64$}\\\hline
		$\tau$ (Polyak parameter) &\multicolumn{3}{|c|}{$0.01$}\\\hline
		$\alpha$ (Entropy Temperature) &\multicolumn{3}{|c|}{$0.01$}\\\hline
	\end{tabular}
	\label{table:hyperparams_sac}
\end{table}
\noindent
