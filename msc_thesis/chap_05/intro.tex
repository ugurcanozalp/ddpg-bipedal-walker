\chapter{CONCLUSION AND FUTURE WORK}
\label{chap:conclusion}

\section{Conclusion}

In this thesis, bipedal robot walking is investigated by deep  reinforcement learning methods. As stated in previous chapters, most of the real world environments are partially observable. In Bipedal-Walker-Hardcore, the environment is also partially observable since agent cannot observe behind of it lacks of acceleration sensors which is better to have for controlling mechanical systems. Therefore, we used Long Short Term Memory and Transformer Neural Networks to capture more information from past observations compared to single instant observation. Along with them, we also implemented Residually connected Feed Forward Neural Network using single instant observation. Moreover, LSTM and Transformer are compared for partially observed RL problems. 

First of all, none of our approaches solved the problem since 300 points required in 100 random simulations as solution. However, our methods partially solved problems by exceeding 200 point limit, while some simulations yield around 280 points in all models. Feed Forward Neural Network seems to be enough for solving the problem, although there exist partial observability in the environment. The robot was able to walk by LSTM model but yield worse results and cannot exceed 120 points in average. Transformer model yield best results by reaching 230 points. 

We believe that these results are not enough to say any model is superior to another, because there are other factors such as DRL method, number of episodes, network size etc. 

Twin Delayed Deep Deterministic Policy Gradient (TD3) 

We were expecting to have better results with xxyy.

\section{Future Work}

