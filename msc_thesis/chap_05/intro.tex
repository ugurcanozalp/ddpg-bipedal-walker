\chapter{CONCLUSION AND FUTURE WORK}
\label{chap:conclusion}
\section{Conclusion}
In this thesis, bipedal robot walking is investigated by deep  reinforcement learning. For real world robot control by RL, simulation is an important step. Usually, models are pretrained in simulation environment before learning in reality due to safety and exploration reasons.  Therefore, our approach is also applicable \\
Why used RL? Complex dynamic models?
TD3?? continious control.
Modifications?
As stated in previous chapters, most of the real world environments are partially observable. In Bipedal-Walker-Hardcore, the environment is also partially observable since agent cannot observe behind of it lacks of acceleration sensors which is better to have for controlling mechanical systems. Therefore, we used Long Short Term Memory and Transformer Neural Networks to capture more information from past observations compared to Residual Feed Forward Neural Network (RFFNN) using single instant observation as input. \\
RFFNN model performed well thanks to carefully selected hyperparameters and modifications on the environment. Since there is no major differences between sequential models (Transformer), the problem is mainly about rewards and exploration, not partial observability. \\
Although LSTM is used commonly for partially observed problems, results are worst compared to other two models, but keeps learning slowly. Without knowing exact reason, it is probably its functional properties. \\
Transformer model worked with best performance among all models. It is surprising because it is not succesfully used in RL problems commonly. In natural language processing, this type of attention models completely replaced recurrent models, and our results seems promising for this in RL domain. \\
\section{Future Work}
First of all, longer observation history can be used to handle partial observability for LSTM and Transformer models. However, this makes learning slower and difficult and requires stable RL algorithms. \\
Secondly, different exploration strategies might be followed. Especially parameter space exploration \cite{plappert_parameter_2018} may perform better since it works better for environments with sparse reward like our environment. \\
Lastly, another RL algorithms might be applied. Especially, stochastic policy models might handle exploration problem since they do not require hyperparameters for exploration, learn randomness during iterations.