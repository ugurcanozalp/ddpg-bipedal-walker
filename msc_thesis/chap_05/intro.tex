\chapter{CONCLUSION AND FUTURE WORK}
\label{chap:conclusion}
\section{Conclusion}
In this thesis, bipedal robot walking is investigated by deep  reinforcement learning methods. As stated in previous chapters, most of the real world environments are partially observable. In Bipedal-Walker-Hardcore, the environment is also partially observable since agent cannot observe behind of it lacks of acceleration sensors which is better to have for controlling mechanical systems. Therefore, we used Long Short Term Memory and Transformer Neural Networks to capture more information from past observations compared to single instant observation. Along with them, we also implemented Residually connected Feed Forward Neural Network using single instant observation. Moreover, LSTM and Transformer are compared for partially observed RL problems. 
\section{Future Work}
First of all, longer observation history can be used to handle partial observability for LSTM and Transformer models. However, this makes learning slower and difficult and requires stable RL algorithms. \\
Secondly, different exploration strategies might be followed. Especially parameter space exploration \cite{plappert_parameter_2018} may perform better since it works better for environments with sparse reward like our environment. \\
Lastly, another RL algorithms might be applied. Especially, stochastic policy models might handle exploration problem since they do not require hyperparameters for exploration, learn randomness during iterations.