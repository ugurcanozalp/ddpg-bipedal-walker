
@misc{shen_text_2003,
	title = {Text {Categorization}},
	url = {https://www.researchgate.net/publication/2894867_Text_Categorization},
	journal = {Encyclopedia of Database Systems},
	author = {Shen, Dou},
	month = oct,
	year = {2003}
}

@book{bishop_pattern_2006,
	title = {Pattern {Recognition} {And} {Machine} {Learning}},
	publisher = {Springer},
	author = {Bishop, Christopher M.},
	year = {2006}
}

@book{duda_pattern_2000,
	address = {605 Third Avenue New York, NY United States},
	edition = {2},
	title = {Pattern {Classification}},
	isbn = {978-0-471-05669-0},
	publisher = {Wiley-Interscience},
	author = {Duda, Richard O.},
	month = oct,
	year = {2000}
}

@book{russell_artificial_nodate,
	edition = {3},
	title = {Artificial {Intelligence} {A} {Modern} {Approach}},
	isbn = {9780136042594},
	publisher = {Prentice Hall},
	author = {Russell, Stuart J. and Norvig, Peter}
}

@inproceedings{joachims_text_1998,
	title = {Text categorization with {Support} {Vector} {Machines}: {Learning} with many relevant features},
	author = {Joachims, Thorsten},
	year = {1998}
}


@article{aci_turkish_2019,
	title = {Turkish {News} {Articles} {Categorization} {Using} {Convolutional} {Neural} {Networks} and {Word2Vec}},
	volume = {12},
	url = {https://dergipark.org.tr/tr/pub/gazibtd/issue/47484/457917},
	doi = {10.17671/gazibtd.457917},
	number = {3},
	author = {Acı, Çiğdem and Çırak, Adem},
	year = {2019},
	pages = {219 -- 228}
}

@article{kilinc_ttc-3600_2017,
	title = {{TTC}-3600: {A} new benchmark dataset for {Turkish} text categorization},
	volume = {43},
	url = {https://www.researchgate.net/publication/286458999_TTC-3600_A_new_benchmark_dataset_for_Turkish_text_categorization},
	doi = {10.1177/0165551515620551},
	number = {2},
	author = {Kılınç, Deniz and Özçift, Akın and Bozyiğit, Fatma and Yıldırım, Pelin and Yücalar, Fatih and Borandağ, Emin},
	month = dec,
	year = {2017},
	pages = {174--185}
}


@inproceedings{joulin_bag_2017,
	title = {Bag of {Tricks} for {Efficient} {Text} {Classification}},
	volume = {2},
	author = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
	month = jan,
	year = {2017}
}

@inproceedings{pennington_glove_2014,
	title = {Glove: {Global} {Vectors} for {Word} {Representation}},
	url = {https://www.researchgate.net/publication/284576917_Glove_Global_Vectors_for_Word_Representation},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christoper D.},
	month = jan,
	year = {2014}
}

@inproceedings{mikolov_efficient_2013,
	title = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
	url = {https://arxiv.org/abs/1301.3781},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = jan,
	year = {2013}
}

@misc{devlin_bert_2018,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for
Language {Understanding}},
	url = {https://arxiv.org/abs/1810.04805},
	language = {en},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = oct,
	year = {2018}
}

@misc{howard_universal_2018,
	title = {Universal {Language} {Model} {Fine}-tuning for {Text} {Classification}},
	url = {https://arxiv.org/abs/1801.06146},
	language = {en},
	author = {Howard, Jeremy and Ruder, Sebastian},
	month = jan,
	year = {2018}
}

@misc{peters_deep_2018,
	title = {Deep contextualized word representations},
	url = {https://arxiv.org/abs/1802.05365},
	language = {en},
	author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
	month = feb,
	year = {2018}
}

@article{mcculloch_logical_1943,
	title = {A logical calculus of the ideas immanent in nervous activity},
	volume = {5},
	issn = {1522-9602},
	url = {https://doi.org/10.1007/BF02478259},
	doi = {10.1007/BF02478259},
	abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
	language = {en},
	number = {4},
	urldate = {2020-06-22},
	journal = {The bulletin of mathematical biophysics},
	author = {McCulloch, Warren S. and Pitts, Walter},
	month = dec,
	year = {1943},
	pages = {115--133}
}

@article{rosenblatt_perceptron_1958,
	title = {The perceptron: {A} probabilistic model for information storage and organization in the brain.},
	volume = {65},
	journal = {Psychological Review},
	author = {Rosenblatt, Frank},
	year = {1958},
	pages = {386--408}
}

@article{cortes_support_1995,
	title = {Support {Vector} {Networks}},
	volume = {20},
	journal = {Machine Learning},
	author = {Cortes, Corrina and Vladimir, Vapnik},
	year = {1995},
	pages = {273--297}
}


@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} {You} {Need}},
	url = {https://arxiv.org/pdf/1706.03762.pdf},
	urldate = {2020-06-16},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	year = {2017}
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667},
	url = {https://www.mitpressjournals.org/doi/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	number = {8},
	urldate = {2020-06-16},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	pages = {1735--1780}
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	copyright = {1986 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/323533a0},
	doi = {10.1038/323533a0},
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	language = {en},
	number = {6088},
	urldate = {2020-06-16},
	journal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	month = oct,
	year = {1986},
	pages = {533--536}
}

@misc{lecun_backpropagation_1989,
	title = {Backpropagation applied to handwritten zip code recognition},
	url = {https://doi.org/10.1162/neco.1989.1.4.541},
	abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
	urldate = {2020-06-16},
	publisher = {MIT Press},
	author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
	month = dec,
	year = {1989}
}

@inproceedings{kaya_sentiment_2012,
	title = {Sentiment {Analysis} of {Turkish} {Political} {News}},
	volume = {1},
	doi = {10.1109/WI-IAT.2012.115},
	abstract = {In this paper, sentiment classification techniques are incorporated into the domain of political news from columns in different Turkish news sites. We compared four supervised machine learning algorithms of Naïve Bayes, Maximum Entropy, SVM and the character based N-Gram Language Model for sentiment classification of Turkish political columns. We also discussed in detail the problem of sentiment classification in the political news domain. We observe from empirical findings that the Maximum Entropy and N-Gram Language Model outperformed the SVM and Naïve Bayes. Using different features, all the approaches reached accuracies of 65\% to 77\%.},
	booktitle = {2012 {IEEE}/{WIC}/{ACM} {International} {Conferences} on {Web} {Intelligence} and {Intelligent} {Agent} {Technology}},
	author = {Kaya, Mesut and Fidan, Güven and Toroslu, Ismail H.},
	month = dec,
	year = {2012},
	keywords = {Bayes methods, Machine Learning, NLP, News Domain, SVM, Sentiment Analysis, Turkish, Turkish news sites, Turkish political columns, Turkish political news, Web sites, character based n-gram language model, learning (artificial intelligence), maximum entropy, maximum entropy methods, naïve Bayes, pattern classification, politics, sentiment analysis, sentiment classification techniques, supervised machine learning algorithms, support vector machines},
	pages = {174--180}
}

@inproceedings{ayata_turkish_2017,
	title = {Turkish tweet sentiment analysis with word embedding and machine learning},
	doi = {10.1109/SIU.2017.7960195},
	abstract = {This work includes processing and classification of tweets which are written in Turkish language. Four different sector tweet datasets are vectorized with Word Embedding model and classified with Support Vector Machine and Random Forests classifiers and results have been compared. We have showed that sector based tweet classification is more successful compared to general tweets. Accuracy rates for Banking sector is 89.97\%, for Football 84.02\%, for Telecom 73.86\%, for Retail 63.68\% and for overall 74.60\% have been achieved.},
	booktitle = {2017 25th {Signal} {Processing} and {Communications} {Applications} {Conference} ({SIU})},
	author = {Ayata, Değer and Saraçlar, Murat and Özgür, Arzucan},
	month = may,
	year = {2017},
	keywords = {Banking, Random Forests, Semantics, Sentiment analysis, Support Vector Machine, Support vector machines, Telecommunications, Text categorization, Turkish language, Turkish tweet sentiment analysis, learning (artificial intelligence), machine learning, natural language processing, pattern classification, random forest classifiers, sentiment analysis, social networking (online), support vector machine, support vector machines, text classification, tweet classification, word embedding, word embedding model, word processing},
	pages = {1--4}
}

@inproceedings{coban_sentiment_2015,
	title = {Sentiment analysis for {Turkish} {Twitter} feeds},
	doi = {10.1109/SIU.2015.7130362},
	abstract = {Sentiment analysis is one of the most useful tools in social media monitoring. Implementing sentiment analysis on data gained from social media (Blogs, Twitter, and Facebook) can increase the customer satisfaction and decrease the costs for a company. Also sentiment analysis can be used in various domains, such as economic, commercial and opinion mining for the users to get meaningful information. In this study, Turkish Twitter feeds collected from Twitter API have been analyzed in terms of the sentiment context whether positive or negative using document classification methods. Experimental results have been conducted on machine learning algorithms such as SVM, Naive Bayes, Multinomial Naive Bayes and KNN. The features represented by vector space are extracted from two different models which are Bag of Words and N-Gram. The experimental results have been investigated on the effect of classification methods.},
	booktitle = {2015 23nd {Signal} {Processing} and {Communications} {Applications} {Conference} ({SIU})},
	author = {Çoban, Önder and Özyer, Barış and Özyer, Gülşah Tümüklü},
	month = may,
	year = {2015},
	note = {ISSN: 2165-0608},
	keywords = {Bayes methods, Blogs, Facebook, KNN, Media, SVM, Sentiment analysis, Support vector machines, Turkish Twitter feeds, Twitter, Twitter API, Uniform resource locators, bag of words, customer satisfaction, data mining, document classification method, feature extraction, learning (artificial intelligence), machine learning, machine learning algorithm, multinomial naive Bayes, n-gram, natural language processing, opinion mining, pattern classification, sentiment analysis, sentiment classification, sentiment context, social media monitoring, social networking (online), support vector machines, text classification, twitter, vector space},
	pages = {2388--2391}
}

@inproceedings{aydogan_turkish_2019,
	title = {Turkish {Text} {Classification} with {Machine} {Learning} and {Transfer} {Learning}},
	doi = {10.1109/IDAP.2019.8875919},
	abstract = {The problem of text classification is one of the most fundamental topics of study in the field of natural language processing, but when reviewing the literature, it is seen that there is an inadequate number of studies for the issue of Turkish text classification. Two different Turkish datasets were created for this aim. Word vectors were created on the first dataset of unlabeled texts. These word vectors were transferred to the second dataset created with data collected from various news sites by transfer learning. Text classification was applied with the machine learning algorithms on this dataset. The effects of transfer learning and transferring of word vectors on the accuracy rate and the performance of machine learning methods were analyzed in detail. When studying the experimental results, it was determined that Support Vector Machine model was performed more successful and It was seen that the accuracy rate was improved with transfer learning.},
	booktitle = {2019 {International} {Artificial} {Intelligence} and {Data} {Processing} {Symposium} ({IDAP})},
	author = {Aydoğan, Murat and Karci, Ali},
	month = sep,
	year = {2019},
	keywords = {Indexes, Machine learning, Machine learning algorithms, Natural language processing, Support Vector Machine model, Support vector machines, Text categorization, Turkish datasets, Turkish text classification, YouTube, learning (artificial intelligence), machine learning, machine learning methods, natural language processing, pattern classification, support vector machines, text analysis, transfer learning, unlabeled texts, word embedding, word vectors},
	pages = {1--6}
}

@inproceedings{kaya_transfer_2013,
	address = {Cham},
	series = {Lecture {Notes} in {Electrical} {Engineering}},
	title = {Transfer {Learning} {Using} {Twitter} {Data} for {Improving} {Sentiment} {Classification} of {Turkish} {Political} {News}},
	isbn = {9783319016047},
	doi = {10.1007/978-3-319-01604-7_14},
	abstract = {In this paper, we aim to determine the overall sentiment classification of Turkish political columns. That is, our goal is to determine whether the whole document has positive or negative opinion regardless of its subject. In order to enhance the performance of the classification, transfer learning is applied from unlabeled Twitter data to labeled political columns. A variation of self-taught learning has been proposed, and implemented for the classification. Different machine learning techniques, including support vector machine, maximum entropy classification, and Naive-Bayes has been used for the supervised learning phase. In our experiments we have obtained up to 26 \% increase in the accuracy of the classification with the inclusion of the Twitter data into the sentiment classification of Turkish political columns using transfer learning.},
	language = {en},
	booktitle = {Information {Sciences} and {Systems} 2013},
	publisher = {Springer International Publishing},
	author = {Kaya, Mesut and Fidan, Guven and Toroslu, I. Hakkı},
	editor = {Gelenbe, Erol and Lent, Ricardo},
	year = {2013},
	keywords = {Feature Ranking , Sentiment Analysis , Support Vector Machine , Transfer Learning , Unlabeled Data },
	pages = {139--148}
}

@inproceedings{liu_multi-label_2020,
	title = {A multi-label text classification model based on
ELMo and attention},
	volume = {309},
	author = {Liu, Wenbin and Wen, Boijan and Gao, Shang and Zheng, Jiesheng and Zheng, Yinlong},
	month = jan,
	year = {2020}
}

@inproceedings{yuksel_turkish_2019,
	title = {Turkish {Tweet} {Classification} with {Transformer} {Encoder}},
	author = {Yüksel, Atıf Emre and Türkmen, Yaşar Alim and Özgür, Arzucan and Altınel, Ayşe Berna},
	month = sep,
	year = {2019},
	pages = {1380--1387}
}

@inproceedings{kilimci_financial_2019,
	address = {Istanbul, Turkey},
	title = {Financial {Sentiment} {Analysis} for {Predicting} {Direction} of {Stocks} using {Bidirectional} {Encoder} {Representations} from {Transformers} ({BERT}) and {Deep} {Learning} {Models}},
	author = {Kilimci, Zeynep Hilal and Othan, Derya},
	month = dec,
	year = {2019}
}


@phdthesis{rysbek_sentiment_2019,
	address = {Ankara},
	title = {Sentiment analysis with recurrent neural networks on turkish reviews domain},
	school = {METU},
	author = {Rysbek, Darkhan and Uğur, Ömür},
	collaborator = {{Middle East Technical University (METU)}},
	year = {2019},
	keywords = {Natural language processing (Computer science), Neural networks (Computer science), Sentimentalism, Turkish language}
}

@phdthesis{abbas_automatic_2019,
	type = {Thesis},
	title = {Automatic {Text} {Categorization} of {Turkish} {News} with {Machine} {Learning} and {Deep} {Learning} {Techniques}},
	url = {http://acikerisim.ybu.edu.tr:8080/xmlui/handle/123456789/1410},
	abstract = {Metin haberlerinin kategorilendirilmesi, haber türlerinin içeriklerine göre sınıflandırılması sürecidir. Bu çalışmada kullanılan Türkçe haber veri setinde, iki tür otomatik öğrenme süreci inşa edilmiştir. Bu iki yöntemden birincisinde, veriler belge biçiminde ve Support Vector Machines SVM makina öğrenmesi kullanılarak sınıflandırılmıştır.İkinci yöntemde etiketlenen haberler Recurrent Neural Network RNN makina öğrenmesi ile sınıflandırılmıştır. Veri setinde, toplam 9000 veri içeren üç haber kategorisi bulunmaktadır. Türkçe karakterler kullanılarak model oluşturulmuştur. Bu modeller eğitilmiş ve test edilmiştir. Hangi modelin daha iyi olduğu, elde edilen sonuçlar karşılaştırılması neticesinde, 0.98 olan RNN modelinin doğruluğunun, 0.96 olan SVM modelinin doğruluğuna göre daha iyi olduğunu göstermiştir, ancak hız oranı SVM, RNN den daha hızlıydı. Biraz zaman gerekirse, metni sınıflandırmak için SVM algoritmasını kullanabiliriz. Sınıflandırmada yüksek doğruluk durumunda, RNN sınıflandırmas metodu kullmıştır.},
	language = {en},
	urldate = {2020-07-20},
	school = {Ankara Yıldırım Beyazıt Üniversitesi Fen Bilimleri Enstitüsü},
	author = {Abbas, Sameer Saeed Ibrahim},
	year = {2019}
}