\begin{thebibliography}{10}

\bibitem{noauthor_bipedalwalker-v2_2021}
{BipedalWalker}-v2, January 2021.

\bibitem{noauthor_bipedalwalkerhardcore-v2_2021}
{BipedalWalkerHardcore}-v2, January 2021.

\bibitem{abbeel_application_2006}
P.~Abbeel, A.~Coates, M.~Quigley, and A.~Ng, An {Application} of
  {Reinforcement} {Learning} to {Aerobatic} {Helicopter} {Flight}, in
  \emph{{NIPS}}, 2006.

\bibitem{brockman_openai_2016}
G.~Brockman, V.~Cheung, L.~Pettersson, J.~Schneider, J.~Schulman, J.~Tang, and
  W.~Zaremba, {OpenAI} {Gym}, arXiv:1606.01540 [cs], June 2016, arXiv:
  1606.01540.

\bibitem{collobert_torch7_2011}
R.~Collobert, K.~Kavukcuoglu, and C.~Farabet, Torch7: {A} {Matlab}-like
  {Environment} for {Machine} {Learning}, 2011.

\bibitem{fris_landing_2020}
R.~Fris, The {Landing} of a {Quadcopter} on {Inclined} {Surfaces} using
  {Reinforcement} {Learning}, 2020.

\bibitem{fu_when_2020}
X.~Fu, F.~Gao, and J.~Wu, When {Do} {Drivers} {Concentrate}? {Attention}-based
  {Driver} {Behavior} {Modeling} {With} {Deep} {Reinforcement} {Learning},
  arXiv:2002.11385 [cs], June 2020, arXiv: 2002.11385.

\bibitem{heess_memory-based_2015}
N.~Heess, J.~J. Hunt, T.~P. Lillicrap, and D.~Silver, Memory-based control with
  recurrent neural networks, arXiv:1512.04455 [cs], December 2015, arXiv:
  1512.04455.

\bibitem{kopsa_reinforcement_2018}
K.~Kopşa and A.~T. Kutay, \emph{Reinforcement learning control for
  autorotation of a simple point-mass helicopter model}, METU, Ankara, 2018.

\bibitem{kumar_bipedal_2018}
A.~Kumar, N.~Paul, and S.~N. Omkar, Bipedal {Walking} {Robot} using {Deep}
  {Deterministic} {Policy} {Gradient}, arXiv:1807.05924 [cs], July 2018, arXiv:
  1807.05924.

\bibitem{mcculloch_logical_1943}
W.~S. McCulloch and W.~Pitts, A logical calculus of the ideas immanent in
  nervous activity, The bulletin of mathematical biophysics, 5(4), pp.
  115--133, December 1943, ISSN 1522-9602.

\bibitem{mitchell_machine_1997}
T.~M. Mitchell, \emph{Machine {Learning}}, McGraw-Hill Education, New York, 1st
  edition edition, March 1997, ISBN 9780070428072.

\bibitem{pan_virtual_2017}
X.~Pan, Y.~You, Z.~Wang, and C.~Lu, Virtual to {Real} {Reinforcement}
  {Learning} for {Autonomous} {Driving}, arXiv:1704.03952 [cs], September 2017,
  arXiv: 1704.03952.

\bibitem{parisotto_stabilizing_2019}
E.~Parisotto, H.~F. Song, J.~W. Rae, R.~Pascanu, C.~Gulcehre, S.~M. Jayakumar,
  M.~Jaderberg, R.~L. Kaufman, A.~Clark, S.~Noury, M.~M. Botvinick, N.~Heess,
  and R.~Hadsell, Stabilizing {Transformers} for {Reinforcement} {Learning},
  arXiv:1910.06764 [cs, stat], October 2019, arXiv: 1910.06764.

\bibitem{paszke_pytorch_2019}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen,
  Z.~Lin, N.~Gimelshein, L.~Antiga, A.~Desmaison, A.~Kopf, E.~Yang, Z.~DeVito,
  M.~Raison, A.~Tejani, S.~Chilamkurthy, B.~Steiner, L.~Fang, J.~Bai, and
  S.~Chintala, {PyTorch}: {An} {Imperative} {Style}, {High}-{Performance}
  {Deep} {Learning} {Library}, Advances in Neural Information Processing
  Systems, 32, pp. 8026--8037, 2019.

\bibitem{rastogi_deep_2017}
D.~Rastogi, Deep {Reinforcement} {Learning} for {Bipedal} {Robots}, 2017.

\bibitem{rosenblatt_perceptron_1958}
F.~Rosenblatt, The perceptron: {A} probabilistic model for information storage
  and organization in the brain., Psychological Review, 65, pp. 386--408, 1958.

\bibitem{rumelhart_learning_1986}
D.~E. Rumelhart, G.~E. Hinton, and R.~J. Williams, Learning representations by
  back-propagating errors, Nature, 323(6088), pp. 533--536, October 1986, ISSN
  1476-4687.

\bibitem{russell_artificial_nodate}
S.~J. Russell and P.~Norvig, \emph{Artificial {Intelligence} {A} {Modern}
  {Approach}}, Prentice Hall, 3 edition, 1995, ISBN 9780136042594.

\bibitem{sallab_deep_2017}
A.~E. Sallab, M.~Abdou, E.~Perot, and S.~Yogamani, Deep {Reinforcement}
  {Learning} framework for {Autonomous} {Driving}, Electronic Imaging,
  2017(19), pp. 70--76, January 2017.

\bibitem{santos_experimental_2012}
S.~R. B.~d. Santos, S.~N. Givigi, and C.~L.~N. Júnior, An experimental
  validation of reinforcement learning applied to the position control of
  {UAVs}, in \emph{2012 {IEEE} {International} {Conference} on {Systems},
  {Man}, and {Cybernetics} ({SMC})}, pp. 2796--2802, October 2012, iSSN:
  1062-922X.

\bibitem{shalev-shwartz_safe_2016}
S.~Shalev-Shwartz, S.~Shammah, and A.~Shashua, Safe, {Multi}-{Agent},
  {Reinforcement} {Learning} for {Autonomous} {Driving}, arXiv:1610.03295 [cs,
  stat], October 2016, arXiv: 1610.03295.

\bibitem{song_recurrent_2018}
D.~R. Song, C.~Yang, C.~McGreavy, and Z.~Li, Recurrent {Deterministic} {Policy}
  {Gradient} {Method} for {Bipedal} {Locomotion} on {Rough} {Terrain}
  {Challenge}, in \emph{2018 15th {International} {Conference} on {Control},
  {Automation}, {Robotics} and {Vision} ({ICARCV})}, pp. 311--318, November
  2018.

\bibitem{sutton_reinforcement_1998}
R.~S. Sutton and A.~G. Barto, \emph{Reinforcement {Learning}: {An}
  {Introduction}}, A Bradford Book, February 1998.

\bibitem{upadhyay_transformer_2019}
U.~Upadhyay, N.~Shah, S.~Ravikanti, and M.~Medhe, Transformer {Based}
  {Reinforcement} {Learning} {For} {Games}, arXiv:1912.03918 [cs], December
  2019, arXiv: 1912.03918.

\bibitem{vaswani_attention_2017}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  L.~Kaiser, and I.~Polosukhin, Attention is {All} {You} {Need}, 2017.

\bibitem{wang_deep_2019}
S.~Wang, D.~Jia, and X.~Weng, Deep {Reinforcement} {Learning} for {Autonomous}
  {Driving}, arXiv:1811.11329 [cs], May 2019, arXiv: 1811.11329.

\bibitem{xiong_layer_2020}
R.~Xiong, Y.~Yang, D.~He, K.~Zheng, S.~Zheng, C.~Xing, H.~Zhang, Y.~Lan,
  L.~Wang, and T.-Y. Liu, On {Layer} {Normalization} in the {Transformer}
  {Architecture}, arXiv:2002.04745 [cs, stat], June 2020, arXiv: 2002.04745.

\end{thebibliography}
