\BOOKMARK [0][-]{chapter*.1}{ABSTRACT}{}% 1
\BOOKMARK [0][-]{chapter*.2}{\326Z}{}% 2
\BOOKMARK [0][-]{chapter*.3}{ACKNOWLEDGMENTS}{}% 3
\BOOKMARK [0][-]{chapter*.4}{TABLE OF CONTENTS}{}% 4
\BOOKMARK [0][-]{chapter*.5}{LIST OF TABLES}{}% 5
\BOOKMARK [0][-]{chapter*.6}{LIST OF FIGURES}{}% 6
\BOOKMARK [0][-]{chapter*.7}{LIST OF ABBREVIATIONS}{}% 7
\BOOKMARK [0][-]{chapter.1}{INTRODUCTION}{}% 8
\BOOKMARK [1][-]{section.1.1}{Problem Statement: Bipedal Walker Robot Control}{chapter.1}% 9
\BOOKMARK [2][-]{subsection.1.1.1}{OpenAI Gym and Bipedal-Walker Environment}{section.1.1}% 10
\BOOKMARK [2][-]{subsection.1.1.2}{Deep Learning Library: PyTorch}{section.1.1}% 11
\BOOKMARK [1][-]{section.1.2}{Proposed Methods and Contribution}{chapter.1}% 12
\BOOKMARK [1][-]{section.1.3}{Related Work}{chapter.1}% 13
\BOOKMARK [2][-]{subsection.1.3.1}{Bipedal Walker}{section.1.3}% 14
\BOOKMARK [2][-]{subsection.1.3.2}{Partially Observable Problems}{section.1.3}% 15
\BOOKMARK [1][-]{section.1.4}{Outline of the Thesis}{chapter.1}% 16
\BOOKMARK [0][-]{chapter.2}{REINFORCEMENT LEARNING}{}% 17
\BOOKMARK [1][-]{section.2.1}{Reinforcement Learning and Optimal Control}{chapter.2}% 18
\BOOKMARK [1][-]{section.2.2}{Differences from Supervised and Unsupervised Learning}{chapter.2}% 19
\BOOKMARK [1][-]{section.2.3}{Markov Decision Process}{chapter.2}% 20
\BOOKMARK [2][-]{subsection.2.3.1}{Model}{section.2.3}% 21
\BOOKMARK [3][-]{subsubsection.2.3.1.1}{Policy}{subsection.2.3.1}% 22
\BOOKMARK [3][-]{subsubsection.2.3.1.2}{Reward}{subsection.2.3.1}% 23
\BOOKMARK [2][-]{subsection.2.3.2}{Return and Discount Factor}{section.2.3}% 24
\BOOKMARK [2][-]{subsection.2.3.3}{Horizon}{section.2.3}% 25
\BOOKMARK [2][-]{subsection.2.3.4}{Value Functions}{section.2.3}% 26
\BOOKMARK [3][-]{subsubsection.2.3.4.1}{State Value Function}{subsection.2.3.4}% 27
\BOOKMARK [3][-]{subsubsection.2.3.4.2}{State-Action Value Function}{subsection.2.3.4}% 28
\BOOKMARK [1][-]{section.2.4}{Model Based Reinforcement Learning}{chapter.2}% 29
\BOOKMARK [1][-]{section.2.5}{Model Free Reinforcement Learning}{chapter.2}% 30
\BOOKMARK [0][-]{chapter.3}{NEURAL NETWORKS AND DEEP LEARNING}{}% 31
\BOOKMARK [1][-]{section.3.1}{Neural Networks}{chapter.3}% 32
\BOOKMARK [2][-]{subsection.3.1.1}{Perceptron}{section.3.1}% 33
\BOOKMARK [2][-]{subsection.3.1.2}{Perceptron as Neural Layer}{section.3.1}% 34
\BOOKMARK [2][-]{subsection.3.1.3}{Feed Forward Neural Networks \(Multilayer Perceptron\)}{section.3.1}% 35
\BOOKMARK [2][-]{subsection.3.1.4}{Convolutional Neural Networks}{section.3.1}% 36
\BOOKMARK [3][-]{subsubsection.3.1.4.1}{Convolution Operation}{subsection.3.1.4}% 37
\BOOKMARK [3][-]{subsubsection.3.1.4.2}{Nonlinear Activation}{subsection.3.1.4}% 38
\BOOKMARK [3][-]{subsubsection.3.1.4.3}{Pooling}{subsection.3.1.4}% 39
\BOOKMARK [2][-]{subsection.3.1.5}{Recurrent Neural Networks}{section.3.1}% 40
\BOOKMARK [3][-]{subsubsection.3.1.5.1}{Long Term Dependence Problem of Vanilla RNNs}{subsection.3.1.5}% 41
\BOOKMARK [3][-]{subsubsection.3.1.5.2}{Long Short Term Memory}{subsection.3.1.5}% 42
\BOOKMARK [2][-]{subsection.3.1.6}{Attention Mechanism}{section.3.1}% 43
\BOOKMARK [3][-]{subsubsection.3.1.6.1}{Transformer}{subsection.3.1.6}% 44
\BOOKMARK [3][-]{subsubsection.3.1.6.2}{Pre-Layer Normalized Transformer}{subsection.3.1.6}% 45
\BOOKMARK [0][-]{chapter.4}{DEEP REINFORCEMENT LEARNING}{}% 46
\BOOKMARK [1][-]{section.4.1}{Deep Reinforcement Learning Algorithms on Continious Action Space}{chapter.4}% 47
\BOOKMARK [2][-]{subsection.4.1.1}{Deep Deterministic Policy Gradient}{section.4.1}% 48
\BOOKMARK [2][-]{subsection.4.1.2}{Twin Delayed Deep Deterministic Policy Gradient}{section.4.1}% 49
\BOOKMARK [1][-]{section.4.2}{Evaluation}{chapter.4}% 50
\BOOKMARK [0][-]{chapter.5}{BIPEDAL WALKING WITH TWIN DELAYED DEEP DETERMINISTIC POLICY GRADIENTS}{}% 51
\BOOKMARK [1][-]{section.5.1}{Details of the Environment}{chapter.5}% 52
\BOOKMARK [1][-]{section.5.2}{RL Method and hyperparameters}{chapter.5}% 53
\BOOKMARK [1][-]{section.5.3}{Proposed Neural Networks}{chapter.5}% 54
\BOOKMARK [2][-]{subsection.5.3.1}{Feed Forward Network}{section.5.3}% 55
\BOOKMARK [2][-]{subsection.5.3.2}{Long Short Term Memory}{section.5.3}% 56
\BOOKMARK [2][-]{subsection.5.3.3}{Transformer \(Pre-layer Normalized\)}{section.5.3}% 57
\BOOKMARK [1][-]{section.5.4}{Results}{chapter.5}% 58
\BOOKMARK [0][-]{chapter.6}{CONCLUSION AND FUTURE WORK}{}% 59
\BOOKMARK [1][-]{section.6.1}{Conclusion}{chapter.6}% 60
\BOOKMARK [1][-]{section.6.2}{Future Work}{chapter.6}% 61
\BOOKMARK [0][-]{chapter*.11}{REFERENCES}{}% 62
\BOOKMARK [0][-]{chapter*.11}{APPENDICES}{}% 63
\BOOKMARK [0][-]{appendix.A}{Proof of Some Theorem}{}% 64
