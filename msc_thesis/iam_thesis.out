\BOOKMARK [0][-]{chapter*.1}{ABSTRACT}{}% 1
\BOOKMARK [0][-]{chapter*.2}{\326Z}{}% 2
\BOOKMARK [0][-]{chapter*.3}{ACKNOWLEDGMENTS}{}% 3
\BOOKMARK [0][-]{chapter*.4}{TABLE OF CONTENTS}{}% 4
\BOOKMARK [0][-]{chapter*.5}{LIST OF TABLES}{}% 5
\BOOKMARK [0][-]{chapter*.6}{LIST OF FIGURES}{}% 6
\BOOKMARK [0][-]{chapter*.7}{LIST OF ABBREVIATIONS}{}% 7
\BOOKMARK [0][-]{chapter.1}{INTRODUCTION}{}% 8
\BOOKMARK [1][-]{section.1.1}{Problem Statement: Bipedal Walker Robot Control}{chapter.1}% 9
\BOOKMARK [2][-]{subsection.1.1.1}{OpenAI Gym and Bipedal-Walker Environment}{section.1.1}% 10
\BOOKMARK [2][-]{subsection.1.1.2}{Deep Learning Library: PyTorch}{section.1.1}% 11
\BOOKMARK [1][-]{section.1.2}{Proposed Methods and Contribution}{chapter.1}% 12
\BOOKMARK [1][-]{section.1.3}{Related Work}{chapter.1}% 13
\BOOKMARK [1][-]{section.1.4}{Outline of the Thesis}{chapter.1}% 14
\BOOKMARK [0][-]{chapter.2}{REINFORCEMENT LEARNING}{}% 15
\BOOKMARK [1][-]{section.2.1}{Reinforcement Learning and Optimal Control}{chapter.2}% 16
\BOOKMARK [1][-]{section.2.2}{Challenges}{chapter.2}% 17
\BOOKMARK [1][-]{section.2.3}{Sequential Decision Making}{chapter.2}% 18
\BOOKMARK [1][-]{section.2.4}{Markov Decision Process}{chapter.2}% 19
\BOOKMARK [2][-]{subsection.2.4.1}{Reward Function}{section.2.4}% 20
\BOOKMARK [2][-]{subsection.2.4.2}{Policy}{section.2.4}% 21
\BOOKMARK [2][-]{subsection.2.4.3}{Optimization Goals}{section.2.4}% 22
\BOOKMARK [1][-]{section.2.5}{Model Based Reinforcement Learning}{chapter.2}% 23
\BOOKMARK [2][-]{subsection.2.5.1}{Dynamic Programming}{section.2.5}% 24
\BOOKMARK [1][-]{section.2.6}{Model Free Reinforcement Learning}{chapter.2}% 25
\BOOKMARK [2][-]{subsection.2.6.1}{Monte Carlo Learning}{section.2.6}% 26
\BOOKMARK [2][-]{subsection.2.6.2}{Temporal Difference Learning}{section.2.6}% 27
\BOOKMARK [3][-]{subsubsection.2.6.2.1}{Q Learning}{subsection.2.6.2}% 28
\BOOKMARK [0][-]{chapter.3}{NEURAL NETWORKS AND DEEP LEARNING}{}% 29
\BOOKMARK [1][-]{section.3.1}{Neural Networks}{chapter.3}% 30
\BOOKMARK [1][-]{section.3.2}{Numerical Optimization}{chapter.3}% 31
\BOOKMARK [1][-]{section.3.3}{Neural Network Types}{chapter.3}% 32
\BOOKMARK [2][-]{subsection.3.3.1}{Perceptron}{section.3.3}% 33
\BOOKMARK [2][-]{subsection.3.3.2}{Feed Forward Neural Networks \(Multilayer Perceptron\)}{section.3.3}% 34
\BOOKMARK [2][-]{subsection.3.3.3}{Recurrent Neural Networks}{section.3.3}% 35
\BOOKMARK [3][-]{subsubsection.3.3.3.1}{Long Term Dependence Problem of Vanilla RNNs}{subsection.3.3.3}% 36
\BOOKMARK [3][-]{subsubsection.3.3.3.2}{Long Short Term Memory}{subsection.3.3.3}% 37
\BOOKMARK [2][-]{subsection.3.3.4}{Attention Mechanism}{section.3.3}% 38
\BOOKMARK [3][-]{subsubsection.3.3.4.1}{Transformer}{subsection.3.3.4}% 39
\BOOKMARK [3][-]{subsubsection.3.3.4.2}{Pre-Layer Normalized Transformer}{subsection.3.3.4}% 40
\BOOKMARK [0][-]{chapter.4}{DEEP REINFORCEMENT LEARNING}{}% 41
\BOOKMARK [1][-]{section.4.1}{Deep Reinforcement Learning Algorithms on Continious Action Space}{chapter.4}% 42
\BOOKMARK [2][-]{subsection.4.1.1}{Deep Q-Learning}{section.4.1}% 43
\BOOKMARK [2][-]{subsection.4.1.2}{Double Deep Q-Learning}{section.4.1}% 44
\BOOKMARK [2][-]{subsection.4.1.3}{Deep Deterministic Policy Gradient}{section.4.1}% 45
\BOOKMARK [2][-]{subsection.4.1.4}{Twin Delayed Deep Deterministic Policy Gradient}{section.4.1}% 46
\BOOKMARK [0][-]{chapter.5}{BIPEDAL WALKING BY TWIN DELAYED DEEP DETERMINISTIC POLICY GRADIENTS}{}% 47
\BOOKMARK [1][-]{section.5.1}{Details of the Environment}{chapter.5}% 48
\BOOKMARK [2][-]{subsection.5.1.1}{Partial Observability}{section.5.1}% 49
\BOOKMARK [1][-]{section.5.2}{RL Method and hyperparameters}{chapter.5}% 50
\BOOKMARK [1][-]{section.5.3}{Proposed Neural Networks}{chapter.5}% 51
\BOOKMARK [2][-]{subsection.5.3.1}{Feed Forward Network}{section.5.3}% 52
\BOOKMARK [2][-]{subsection.5.3.2}{Long Short Term Memory}{section.5.3}% 53
\BOOKMARK [2][-]{subsection.5.3.3}{Transformer \(Pre-layer Normalized\)}{section.5.3}% 54
\BOOKMARK [1][-]{section.5.4}{Results}{chapter.5}% 55
\BOOKMARK [0][-]{chapter.6}{CONCLUSION AND FUTURE WORK}{}% 56
\BOOKMARK [1][-]{section.6.1}{Conclusion}{chapter.6}% 57
\BOOKMARK [1][-]{section.6.2}{Future Work}{chapter.6}% 58
\BOOKMARK [0][-]{chapter*.14}{REFERENCES}{}% 59
\BOOKMARK [0][-]{chapter*.14}{APPENDICES}{}% 60
\BOOKMARK [0][-]{appendix.A}{Proof of Some Theorem}{}% 61
