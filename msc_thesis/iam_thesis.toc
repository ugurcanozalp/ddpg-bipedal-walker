\babel@toc {english}{}
\babel@toc {english}{}
\contentsline {leads}{\numberline {}ABSTRACT}{vii}{chapter*.1}%
\babel@toc {english}{}
\babel@toc {turkish}{}
\contentsline {leads}{\numberline {}\"OZ}{ix}{chapter*.2}%
\babel@toc {english}{}
\contentsline {leads}{\numberline {}ACKNOWLEDGMENTS}{xi}{chapter*.3}%
\contentsline {leads}{\numberline {}TABLE OF CONTENTS}{xiii}{chapter*.4}%
\contentsline {leads}{\numberline {}LIST OF TABLES}{xvii}{chapter*.5}%
\contentsline {leads}{\numberline {}LIST OF FIGURES}{xviii}{chapter*.6}%
\contentsline {leads}{\numberline {}LIST OF ABBREVIATIONS}{xix}{chapter*.7}%
\numberline {}CHAPTERS
\babel@toc {turkish}{}
\babel@toc {english}{}
\babel@toc {english}{}
\babel@toc {english}{}
\contentsline {chapter}{\numberline {1}\MakeUppercase {INTRODUCTION}}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Problem Statement: Bipedal Walker Robot Control}{2}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}OpenAI Gym and Bipedal-Walker Environment}{2}{subsection.1.1.1}%
\contentsline {subsection}{\numberline {1.1.2}Deep Learning Library: PyTorch}{3}{subsection.1.1.2}%
\contentsline {section}{\numberline {1.2}Proposed Methods and Contribution}{4}{section.1.2}%
\contentsline {section}{\numberline {1.3}Related Work}{4}{section.1.3}%
\contentsline {section}{\numberline {1.4}Outline of the Thesis}{5}{section.1.4}%
\contentsline {chapter}{\numberline {2}\MakeUppercase {REINFORCEMENT LEARNING}}{7}{chapter.2}%
\contentsline {section}{\numberline {2.1}Reinforcement Learning and Optimal Control}{8}{section.2.1}%
\contentsline {section}{\numberline {2.2}Challenges}{9}{section.2.2}%
\contentsline {section}{\numberline {2.3}Sequential Decision Making}{10}{section.2.3}%
\contentsline {section}{\numberline {2.4}Markov Decision Process}{10}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}Reward Function}{10}{subsection.2.4.1}%
\contentsline {subsection}{\numberline {2.4.2}Policy}{11}{subsection.2.4.2}%
\contentsline {subsection}{\numberline {2.4.3}Optimization Goals}{11}{subsection.2.4.3}%
\contentsline {section}{\numberline {2.5}Model Based Reinforcement Learning}{12}{section.2.5}%
\contentsline {subsection}{\numberline {2.5.1}Dynamic Programming}{12}{subsection.2.5.1}%
\contentsline {section}{\numberline {2.6}Model Free Reinforcement Learning}{12}{section.2.6}%
\contentsline {subsection}{\numberline {2.6.1}Monte Carlo Learning}{12}{subsection.2.6.1}%
\contentsline {subsection}{\numberline {2.6.2}Temporal Difference Learning}{12}{subsection.2.6.2}%
\contentsline {subsubsection}{\numberline {2.6.2.1}Q Learning}{12}{subsubsection.2.6.2.1}%
\contentsline {chapter}{\numberline {3}\MakeUppercase {NEURAL NETWORKS AND DEEP LEARNING}}{13}{chapter.3}%
\contentsline {section}{\numberline {3.1}Neural Networks}{13}{section.3.1}%
\contentsline {section}{\numberline {3.2}Numerical Optimization}{13}{section.3.2}%
\contentsline {section}{\numberline {3.3}Neural Network Types}{14}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}Perceptron}{14}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2}Feed Forward Neural Networks (Multilayer Perceptron)}{14}{subsection.3.3.2}%
\contentsline {subsection}{\numberline {3.3.3}Recurrent Neural Networks}{15}{subsection.3.3.3}%
\contentsline {subsubsection}{\numberline {3.3.3.1}Long Term Dependence Problem of Vanilla RNNs}{16}{subsubsection.3.3.3.1}%
\contentsline {subsubsection}{\numberline {3.3.3.2}Long Short Term Memory}{16}{subsubsection.3.3.3.2}%
\contentsline {subsection}{\numberline {3.3.4}Attention Mechanism}{17}{subsection.3.3.4}%
\contentsline {subsubsection}{\numberline {3.3.4.1}Transformer}{18}{subsubsection.3.3.4.1}%
\contentsline {subsubsection}{\numberline {3.3.4.2}Pre-Layer Normalized Transformer}{20}{subsubsection.3.3.4.2}%
\contentsline {chapter}{\numberline {4}\MakeUppercase {DEEP REINFORCEMENT LEARNING}}{23}{chapter.4}%
\contentsline {section}{\numberline {4.1}Deep Reinforcement Learning Algorithms on Continious Action Space}{23}{section.4.1}%
\contentsline {subsection}{\numberline {4.1.1}Deep Q-Learning}{23}{subsection.4.1.1}%
\contentsline {subsection}{\numberline {4.1.2}Double Deep Q-Learning}{23}{subsection.4.1.2}%
\contentsline {subsection}{\numberline {4.1.3}Deep Deterministic Policy Gradient}{23}{subsection.4.1.3}%
\contentsline {subsection}{\numberline {4.1.4}Twin Delayed Deep Deterministic Policy Gradient}{23}{subsection.4.1.4}%
\contentsline {chapter}{\numberline {5}\MakeUppercase {BIPEDAL WALKING BY TWIN DELAYED DEEP DETERMINISTIC POLICY GRADIENTS}}{25}{chapter.5}%
\contentsline {section}{\numberline {5.1}Details of the Environment}{25}{section.5.1}%
\contentsline {subsection}{\numberline {5.1.1}Partial Observability}{26}{subsection.5.1.1}%
\contentsline {section}{\numberline {5.2}RL Method and hyperparameters}{26}{section.5.2}%
\contentsline {section}{\numberline {5.3}Proposed Neural Networks}{26}{section.5.3}%
\contentsline {subsection}{\numberline {5.3.1}Feed Forward Network}{27}{subsection.5.3.1}%
\contentsline {subsection}{\numberline {5.3.2}Long Short Term Memory}{27}{subsection.5.3.2}%
\contentsline {subsection}{\numberline {5.3.3}Transformer (Pre-layer Normalized)}{27}{subsection.5.3.3}%
\contentsline {section}{\numberline {5.4}Results}{27}{section.5.4}%
\contentsline {chapter}{\numberline {6}\MakeUppercase {CONCLUSION AND FUTURE WORK}}{29}{chapter.6}%
\contentsline {section}{\numberline {6.1}Conclusion}{29}{section.6.1}%
\contentsline {section}{\numberline {6.2}Future Work}{29}{section.6.2}%
\contentsline {leads}{\numberline {}\uppercase {REFERENCES}}{31}{chapter*.14}%
\contentsline {nodots}{\numberline {}APPENDICES}{33}{chapter*.14}%
\contentsline {chapter}{\numberline {A}\MakeUppercase {Proof of Some Theorem}}{35}{appendix.A}%
