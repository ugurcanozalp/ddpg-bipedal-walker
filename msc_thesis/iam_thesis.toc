\babel@toc {english}{}
\babel@toc {english}{}
\contentsline {leads}{\numberline {}ABSTRACT}{vii}{chapter*.1}%
\babel@toc {english}{}
\babel@toc {turkish}{}
\contentsline {leads}{\numberline {}\"OZ}{ix}{chapter*.2}%
\babel@toc {english}{}
\contentsline {leads}{\numberline {}ACKNOWLEDGMENTS}{xi}{chapter*.3}%
\contentsline {leads}{\numberline {}TABLE OF CONTENTS}{xiii}{chapter*.4}%
\contentsline {leads}{\numberline {}LIST OF TABLES}{xvii}{chapter*.5}%
\contentsline {leads}{\numberline {}LIST OF FIGURES}{xviii}{chapter*.6}%
\contentsline {leads}{\numberline {}LIST OF ABBREVIATIONS}{xix}{chapter*.7}%
\numberline {}CHAPTERS
\babel@toc {turkish}{}
\babel@toc {english}{}
\babel@toc {english}{}
\babel@toc {english}{}
\contentsline {chapter}{\numberline {1}\MakeUppercase {INTRODUCTION}}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Problem Statement: Bipedal Walker Robot Control}{2}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}OpenAI Gym and Bipedal-Walker Environment}{2}{subsection.1.1.1}%
\contentsline {subsection}{\numberline {1.1.2}Deep Learning Library: PyTorch}{3}{subsection.1.1.2}%
\contentsline {section}{\numberline {1.2}Proposed Methods and Contribution}{4}{section.1.2}%
\contentsline {section}{\numberline {1.3}Related Work}{4}{section.1.3}%
\contentsline {subsection}{\numberline {1.3.1}Bipedal Walker}{4}{subsection.1.3.1}%
\contentsline {subsection}{\numberline {1.3.2}Partially Observable Problems}{4}{subsection.1.3.2}%
\contentsline {section}{\numberline {1.4}Outline of the Thesis}{4}{section.1.4}%
\contentsline {chapter}{\numberline {2}\MakeUppercase {REINFORCEMENT LEARNING}}{5}{chapter.2}%
\contentsline {section}{\numberline {2.1}Reinforcement Learning and Optimal Control}{5}{section.2.1}%
\contentsline {section}{\numberline {2.2}Differences from Supervised and Unsupervised Learning}{6}{section.2.2}%
\contentsline {section}{\numberline {2.3}Markov Decision Process}{6}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Model}{6}{subsection.2.3.1}%
\contentsline {subsubsection}{\numberline {2.3.1.1}Policy}{7}{subsubsection.2.3.1.1}%
\contentsline {subsubsection}{\numberline {2.3.1.2}Reward}{7}{subsubsection.2.3.1.2}%
\contentsline {subsection}{\numberline {2.3.2}Return and Discount Factor}{7}{subsection.2.3.2}%
\contentsline {subsection}{\numberline {2.3.3}Horizon}{7}{subsection.2.3.3}%
\contentsline {subsection}{\numberline {2.3.4}Value Functions}{7}{subsection.2.3.4}%
\contentsline {subsubsection}{\numberline {2.3.4.1}State Value Function}{7}{subsubsection.2.3.4.1}%
\contentsline {subsubsection}{\numberline {2.3.4.2}State-Action Value Function}{7}{subsubsection.2.3.4.2}%
\contentsline {section}{\numberline {2.4}Model Based Reinforcement Learning}{7}{section.2.4}%
\contentsline {section}{\numberline {2.5}Model Free Reinforcement Learning}{7}{section.2.5}%
\contentsline {chapter}{\numberline {3}\MakeUppercase {NEURAL NETWORKS AND DEEP LEARNING}}{9}{chapter.3}%
\contentsline {section}{\numberline {3.1}Neural Networks}{9}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}Perceptron}{9}{subsection.3.1.1}%
\contentsline {subsection}{\numberline {3.1.2}Perceptron as Neural Layer}{10}{subsection.3.1.2}%
\contentsline {subsection}{\numberline {3.1.3}Feed Forward Neural Networks (Multilayer Perceptron)}{10}{subsection.3.1.3}%
\contentsline {subsection}{\numberline {3.1.4}Convolutional Neural Networks}{11}{subsection.3.1.4}%
\contentsline {subsubsection}{\numberline {3.1.4.1}Convolution Operation}{11}{subsubsection.3.1.4.1}%
\contentsline {subsubsection}{\numberline {3.1.4.2}Nonlinear Activation}{11}{subsubsection.3.1.4.2}%
\contentsline {subsubsection}{\numberline {3.1.4.3}Pooling}{12}{subsubsection.3.1.4.3}%
\contentsline {subsection}{\numberline {3.1.5}Recurrent Neural Networks}{12}{subsection.3.1.5}%
\contentsline {subsubsection}{\numberline {3.1.5.1}Long Term Dependence Problem of Vanilla RNNs}{12}{subsubsection.3.1.5.1}%
\contentsline {subsubsection}{\numberline {3.1.5.2}Long Short Term Memory}{13}{subsubsection.3.1.5.2}%
\contentsline {subsection}{\numberline {3.1.6}Attention Mechanism}{14}{subsection.3.1.6}%
\contentsline {subsubsection}{\numberline {3.1.6.1}Transformer}{15}{subsubsection.3.1.6.1}%
\contentsline {subsubsection}{\numberline {3.1.6.2}Pre-Layer Normalized Transformer}{17}{subsubsection.3.1.6.2}%
\contentsline {chapter}{\numberline {4}\MakeUppercase {DEEP REINFORCEMENT LEARNING}}{19}{chapter.4}%
\contentsline {section}{\numberline {4.1}Deep Reinforcement Learning Algorithms on Continious Action Space}{19}{section.4.1}%
\contentsline {subsection}{\numberline {4.1.1}Deep Deterministic Policy Gradient}{19}{subsection.4.1.1}%
\contentsline {subsection}{\numberline {4.1.2}Twin Delayed Deep Deterministic Policy Gradient}{19}{subsection.4.1.2}%
\contentsline {section}{\numberline {4.2}Evaluation}{19}{section.4.2}%
\contentsline {chapter}{\numberline {5}\MakeUppercase {BIPEDAL WALKING WITH TWIN DELAYED DEEP DETERMINISTIC POLICY GRADIENTS}}{21}{chapter.5}%
\contentsline {section}{\numberline {5.1}Details of the Environment}{21}{section.5.1}%
\contentsline {section}{\numberline {5.2}RL Method and hyperparameters}{22}{section.5.2}%
\contentsline {section}{\numberline {5.3}Proposed Neural Networks}{22}{section.5.3}%
\contentsline {subsection}{\numberline {5.3.1}Feed Forward Network}{22}{subsection.5.3.1}%
\contentsline {subsection}{\numberline {5.3.2}Long Short Term Memory}{22}{subsection.5.3.2}%
\contentsline {subsection}{\numberline {5.3.3}Transformer (Pre-layer Normalized)}{22}{subsection.5.3.3}%
\contentsline {section}{\numberline {5.4}Results}{22}{section.5.4}%
\contentsline {chapter}{\numberline {6}\MakeUppercase {CONCLUSION AND FUTURE WORK}}{23}{chapter.6}%
\contentsline {section}{\numberline {6.1}Conclusion}{23}{section.6.1}%
\contentsline {section}{\numberline {6.2}Future Work}{23}{section.6.2}%
\contentsline {leads}{\numberline {}\uppercase {REFERENCES}}{25}{chapter*.11}%
\contentsline {nodots}{\numberline {}APPENDICES}{25}{chapter*.11}%
\contentsline {chapter}{\numberline {A}\MakeUppercase {Proof of Some Theorem}}{27}{appendix.A}%
